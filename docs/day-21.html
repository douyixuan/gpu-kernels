<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 21 - GPU Kernels Learning Journey</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 1rem;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .header .subtitle {
            font-size: 1.1rem;
            opacity: 0.9;
        }
        
        .nav {
            background: white;
            padding: 1rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }
        
        .nav a {
            color: #667eea;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: all 0.3s;
        }
        
        .nav a:hover {
            background: #667eea;
            color: white;
        }
        
        .container {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 1rem;
        }
        
        .description {
            background: white;
            padding: 2rem;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            margin-bottom: 2rem;
        }
        
        .description h2 {
            color: #667eea;
            margin-bottom: 1rem;
            border-bottom: 2px solid #667eea;
            padding-bottom: 0.5rem;
        }
        
        .description pre {
            background: #f5f5f5;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        .file-section {
            background: white;
            margin-bottom: 2rem;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        
        .file-section h3 {
            background: #667eea;
            color: white;
            padding: 1rem;
            margin: 0;
        }
        
        .file-section pre {
            margin: 0;
            border-radius: 0;
        }
        
        .file-section code {
            display: block;
            padding: 1.5rem;
            max-height: 600px;
            overflow: auto;
        }
        
        .no-files {
            text-align: center;
            padding: 3rem;
            color: #999;
            font-style: italic;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8rem;
            }
            
            .nav-content {
                flex-direction: column;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Day 21</h1>
        <div class="subtitle">GPU Kernels Learning Journey</div>
    </div>
    
    <nav class="nav">
        <div class="nav-content">
            <a href="index.html">← Back to Index</a>
            <div>
                <a href="day-20.html">← Day 20</a>
                <a href="day-22.html">Day 22 →</a>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="description">
            <h2>Description</h2>
            <div class="desc-content">
<h3>File: <code>cuda_sgc.cu</code></h3>
<p><strong>Summary:</strong><br />
Implemented a Stochastic Gradient Descent (SGD) algorithm in CUDA, focusing on optimizing calculations via shared memory and other performance tricks. This implementation is designed for linear regression, consisting of weight and bias updates, where the effectiveness of using CUDA's parallel computation capabilities significantly enhances the training speed.</p>
<p><strong>Key Components:</strong>
1. <strong>Compute Loss:</strong><br />
   - The kernel <code>compute_loss</code> calculates the predictions and corresponding squared loss for each data point. The predictions are calculated using an input matrix <code>X</code>, weights <code>W</code>, and bias <code>b</code>.</p>
<ol>
<li><strong>Compute Gradients:</strong>  </li>
<li>
<p>The <code>compute_gradients</code> kernel computes the gradients of the loss with respect to weights and bias. It uses shared memory (<code>db_shared</code>) for the computation of bias gradients, reducing global memory access and improving performance.</p>
</li>
<li>
<p><strong>Update Weights:</strong>  </p>
</li>
<li>
<p>The <code>update_weights</code> kernel updates the weights and bias based on the calculated gradients and the learning rate.</p>
</li>
<li>
<p><strong>Training Function:</strong>  </p>
</li>
<li><code>train_sgd</code> orchestrates the memory management, kernel launches, and data transfers between host and device for the entire training loop over a specified number of epochs.</li>
</ol>
<hr />
<h3>Reading:</h3>
<ul>
<li>Continued the exploration of optimization techniques for CUDA applications, focusing on strategies for improving kernel performance and reducing latency.</li>
<li>Reviewed literature on best practices for applying SGD and other optimization algorithms in machine learning frameworks, considering both theoretical and practical aspects.</li>
</ul>
<hr />
            </div>
        </div>
        
        <h2 style="margin-bottom: 1rem; color: #667eea;">Code Files</h2>

    <div class="file-section">
        <h3>cuda_sgc.cu</h3>
        <pre><code class="language-cuda">
#include &lt;iostream&gt;
#include &lt;cuda_runtime.h&gt;
#include &lt;curand_kernel.h&gt;

#define BLOCK_SIZE 256

// CUDA kernel to compute predictions and squared loss
__global__ void compute_loss(float* X, float* y, float* W, float* b, float* loss, float* y_pred, int N, int D) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; N) {  
        float y_pred_val = 0.0f;
        for (int i = 0; i &lt; D; i++) {
            y_pred_val += X[idx * D + i] * W[i];
        }
        y_pred_val += *b; // Use single scalar bias
        y_pred[idx] = y_pred_val;
        loss[idx] = (y[idx] - y_pred_val) * (y[idx] - y_pred_val); // Squared loss
    }
}

// CUDA kernel to compute gradients
__global__ void compute_gradients(float* X, float* loss, float* dW, float* db, int N, int D) {
    __shared__ float db_shared[BLOCK_SIZE];
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx &lt; D) {
        float gradW = 0.0f;
        for (int i = 0; i &lt; N; i++) {
            gradW += X[i * D + idx] * loss[i];
        }
        dW[idx] = - (2.0f / N) * gradW;
    }
    float gradb = 0.0f;
    if (idx &lt; N) {
        gradb = loss[idx];
    }
    db_shared[threadIdx.x] = gradb;
    __syncthreads();

    if (threadIdx.x == 0) {
        float sum_db = 0.0f;
        for (int i = 0; i &lt; blockDim.x; i++) {
            sum_db += db_shared[i];
        }
        atomicAdd(db, - (2.0f / N) * sum_db);
    }
}

// CUDA kernel to update weights using SGD
__global__ void update_weights(float* W, float* dW, float* b, float* db, float lr, int D) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; D) {
        W[idx] -= lr * dW[idx];
    }
    if (idx == 0) {
        *b -= lr * (*db);
    }
}

// Host function to train the model
void train_sgd(float* h_X, float* h_y, float* h_W, float* h_b, int N, int D, float lr, int epochs) {
    float *d_X, *d_y, *d_W, *d_b, *d_gradW, *d_gradb, *d_loss, *d_y_pred;
    cudaMalloc(&amp;d_X, N * D * sizeof(float));
    cudaMalloc(&amp;d_y, N * sizeof(float));
    cudaMalloc(&amp;d_W, D * sizeof(float));
    cudaMalloc(&amp;d_b, sizeof(float));
    cudaMalloc(&amp;d_gradW, D * sizeof(float));
    cudaMalloc(&amp;d_gradb, sizeof(float));
    cudaMalloc(&amp;d_loss, N * sizeof(float));
    cudaMalloc(&amp;d_y_pred, N * sizeof(float));
    
    cudaMemcpy(d_X, h_X, N * D * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_y, h_y, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_W, h_W, D * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b, sizeof(float), cudaMemcpyHostToDevice);
    
    int blocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;
    int blocks_grad = (D + BLOCK_SIZE - 1) / BLOCK_SIZE;

    for (int epoch = 0; epoch &lt; epochs; ++epoch) {
        compute_loss&lt;&lt;&lt;blocks, BLOCK_SIZE&gt;&gt;&gt;(d_X, d_y, d_W, d_b, d_loss, d_y_pred, N, D);
        cudaDeviceSynchronize();

        compute_gradients&lt;&lt;&lt;blocks_grad, BLOCK_SIZE&gt;&gt;&gt;(d_X, d_loss, d_gradW, d_gradb, N, D);
        cudaDeviceSynchronize();

        update_weights&lt;&lt;&lt;blocks_grad, BLOCK_SIZE&gt;&gt;&gt;(d_W, d_gradW, d_b, d_gradb, lr, D);
        cudaDeviceSynchronize();
    }

    cudaMemcpy(h_W, d_W, D * sizeof(float), cudaMemcpyDeviceToHost);
    cudaMemcpy(h_b, d_b, sizeof(float), cudaMemcpyDeviceToHost);
    
    cudaFree(d_X);
    cudaFree(d_y);
    cudaFree(d_W);
    cudaFree(d_b);
    cudaFree(d_gradW);
    cudaFree(d_gradb);
    cudaFree(d_loss);
    cudaFree(d_y_pred);
}

int main() {
    int N = 1024;
    int D = 10;
    float lr = 0.01;
    int epochs = 1000;

    float *h_X = new float[N * D];
    float *h_y = new float[N];
    float *h_W = new float[D];
    float *h_b = new float[1];

    srand(42);
    for (int i = 0; i &lt; N * D; i++) {
        h_X[i] = static_cast&lt;float&gt;(rand()) / RAND_MAX;
    }
    for (int i = 0; i &lt; N; i++) {
        h_y[i] = static_cast&lt;float&gt;(rand()) / RAND_MAX;
    }
    for (int i = 0; i &lt; D; i++) {
        h_W[i] = static_cast&lt;float&gt;(rand()) / RAND_MAX;
    }
    *h_b = static_cast&lt;float&gt;(rand()) / RAND_MAX;

    train_sgd(h_X, h_y, h_W, h_b, N, D, lr, epochs);

    std::cout &lt;&lt; "Trained Weights: ";
    for (int i = 0; i &lt; D; i++) std::cout &lt;&lt; h_W[i] &lt;&lt; " ";
    std::cout &lt;&lt; "\nTrained Bias: " &lt;&lt; *h_b &lt;&lt; std::endl;

    delete[] h_X;
    delete[] h_y;
    delete[] h_W;
    delete[] h_b;
    return 0;
}
</code></pre>
    </div>

    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-markdown.min.js"></script>
</body>
</html>