<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 10 - GPU Kernels Learning Journey</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 1rem;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .header .subtitle {
            font-size: 1.1rem;
            opacity: 0.9;
        }
        
        .nav {
            background: white;
            padding: 1rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }
        
        .nav a {
            color: #667eea;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: all 0.3s;
        }
        
        .nav a:hover {
            background: #667eea;
            color: white;
        }
        
        .container {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 1rem;
        }
        
        .description {
            background: white;
            padding: 2rem;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            margin-bottom: 2rem;
        }
        
        .description h2 {
            color: #667eea;
            margin-bottom: 1rem;
            border-bottom: 2px solid #667eea;
            padding-bottom: 0.5rem;
        }
        
        .description pre {
            background: #f5f5f5;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        .file-section {
            background: white;
            margin-bottom: 2rem;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        
        .file-section h3 {
            background: #667eea;
            color: white;
            padding: 1rem;
            margin: 0;
        }
        
        .file-section pre {
            margin: 0;
            border-radius: 0;
        }
        
        .file-section code {
            display: block;
            padding: 1.5rem;
            max-height: 600px;
            overflow: auto;
        }
        
        .no-files {
            text-align: center;
            padding: 3rem;
            color: #999;
            font-style: italic;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8rem;
            }
            
            .nav-content {
                flex-direction: column;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Day 10</h1>
        <div class="subtitle">GPU Kernels Learning Journey</div>
    </div>
    
    <nav class="nav">
        <div class="nav-content">
            <a href="index.html">← Back to Index</a>
            <div>
                <a href="day-9.html">← Day 9</a>
                <a href="day-11.html">Day 11 →</a>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="description">
            <h2>Description</h2>
            <div class="desc-content">
<p>No description available for this day.</p>
            </div>
        </div>
        
        <h2 style="margin-bottom: 1rem; color: #667eea;">Code Files</h2>

    <div class="file-section">
        <h3>torch_test.py</h3>
        <pre><code class="language-python">import torch
import torch.nn.functional as F

# Define Q, K, V, and expected O matrices
def load_csv_to_tensor(file_path):
    df = pd.read_csv(file_path, header=None)  # Assuming no header in the CSV
    tensor = torch.tensor(df.values)  # Convert DataFrame to tensor
    return tensor
# Load the matrices
Q = load_csv_to_tensor('query_output.csv')
K = load_csv_to_tensor('key_output.csv')
V = load_csv_to_tensor('value_output.csv')
expected_O = load_csv_to_tensor('output_output.csv')
# Step 1: Scaled Dot-Product Attention Calculation
# Compute the scaled dot-product of Q and K^T
dk = Q.size(-1)  # Dimension of the embedding (key size)
scores = torch.matmul(Q, K.T) / (dk ** 0.5)  # Scaling factor by sqrt(d_k)

# To prevent instability, apply softmax in a numerically stable way
attention_weights = F.softmax(scores, dim=-1)

# Step 2: Multiply attention weights with V to compute O
computed_O = torch.matmul(attention_weights, V)

# Step 3: Compare computed_O with expected_O
is_close = torch.allclose(computed_O, expected_O, atol=1e-5)

# Print results
print("Attention Weights:")
print(attention_weights)
print("Computed Output O:")
print(computed_O)
print("Expected Output O:")
print(expected_O)
print("Do the computed output and expected output match (within tolerance)?", is_close)

# Debugging: Exploring potential issues in scores
print("Raw scores (Q @ K.T):")
print(scores)
print("Max score (for stability):", torch.max(scores))
</code></pre>
    </div>

    <div class="file-section">
        <h3>flash_attention_forward.cu</h3>
        <pre><code class="language-cuda">#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;
#include &lt;iostream&gt;
#include &lt;random&gt;
#include &lt;cmath&gt;
#include &lt;curand.h&gt; 
#include &lt;fstream&gt;




__global__
void forward_kernel(const float* query_matrix_device_pointer, const float* key_matrix_device_pointer, const float* value_matrix_device_pointer, const int sequence_length, const int embedding_dimension,
                    const int total_columns_in_blocks, const int total_rows_in_blocks, const int block_size_columns, const int block_size_rows, const float softmax_scale,
                    float* sum_matrix_device_pointer, float *max_matrix_device_pointer, float* output_matrix_device_pointer) {
    int thread_index_x = threadIdx.x;
    int block_index_x = blockIdx.x; 
    int block_index_y = blockIdx.y;  // batch and head index

    // Offset into query_matrix_device_pointer,key_matrix_device_pointer,value_matrix_device_pointer,output_matrix_device_pointer,sum_matrix_device_pointer,max_matrix_device_pointer - different for each batch and head
    int qkv_offset = (block_index_x * gridDim.y * sequence_length * embedding_dimension) + (block_index_y * sequence_length * embedding_dimension);  // gridDim.y = num_heads
    int lm_offset = (block_index_x * gridDim.y * sequence_length) + (block_index_y * sequence_length);  // offset for sum_matrix_device_pointer and max_matrix_device_pointer

    // Define SRAM for Q,K,V,S
    extern __shared__ float shared_memory[];
    int tile_size = block_size_columns * embedding_dimension;  // size of query_matrix_tile, key_matrix_tile, value_matrix_tile
    float* query_matrix_tile = shared_memory;
    float* key_matrix_tile = &amp;shared_memory[tile_size];
    float* value_matrix_tile = &amp;shared_memory[tile_size * 2];
    float* score_matrix_tile = &amp;shared_memory[tile_size * 3];
    float eps=1e-10;
    for (int column_block_index = 0; column_block_index &lt; total_columns_in_blocks; column_block_index++) {

        // Load key_matrix_tile, value_matrix_tile to SRAM
        for (int embedding_index = 0; embedding_index &lt; embedding_dimension; embedding_index++) {
            key_matrix_tile[(thread_index_x * embedding_dimension) + embedding_index] = key_matrix_device_pointer[qkv_offset + (tile_size * column_block_index) + (thread_index_x * embedding_dimension) + embedding_index];
            value_matrix_tile[(thread_index_x * embedding_dimension) + embedding_index] = value_matrix_device_pointer[qkv_offset + (tile_size * column_block_index) + (thread_index_x * embedding_dimension) + embedding_index];
        }
        __syncthreads();  

        for (int row_block_index = 0; row_block_index &lt; total_rows_in_blocks; row_block_index++)  {

            
            for (int embedding_index = 0; embedding_index &lt; embedding_dimension; embedding_index++) {
                query_matrix_tile[(thread_index_x * embedding_dimension) + embedding_index] = query_matrix_device_pointer[qkv_offset + (tile_size * row_block_index) + (thread_index_x * embedding_dimension) + embedding_index];
            }
            float row_max_previous = max_matrix_device_pointer[lm_offset + (block_size_rows * row_block_index) + thread_index_x];
            float row_sum_previous = sum_matrix_device_pointer[lm_offset + (block_size_rows * row_block_index) + thread_index_x];

            
            float row_max = -INFINITY;
            for (int column_index_inner = 0; column_index_inner &lt; block_size_columns; column_index_inner++) {
                float sum = 0;
                for (int embedding_index = 0; embedding_index &lt; embedding_dimension; embedding_index++) {
                    sum += query_matrix_tile[(thread_index_x * embedding_dimension) + embedding_index] * key_matrix_tile[(column_index_inner * embedding_dimension) + embedding_index];
                }
                sum *= softmax_scale;
                score_matrix_tile[(block_size_columns * thread_index_x) + column_index_inner] = sum;

                if (sum &gt; row_max)
                    row_max = sum;
            }

            // probability_matrix_tile = exp(score_matrix_tile - row_max), row_sum = rowsum(probability_matrix_tile)
            float row_sum = 0;
            for (int column_index_inner = 0; column_index_inner &lt; block_size_columns; column_index_inner++) {
                score_matrix_tile[(block_size_columns * thread_index_x) + column_index_inner] = __expf(score_matrix_tile[(block_size_columns * thread_index_x) + column_index_inner] - row_max);
                row_sum += score_matrix_tile[(block_size_columns * thread_index_x) + column_index_inner];
            }

            float row_max_new = max(row_max_previous, row_max);
            float row_sum_new = (__expf(row_max_previous - row_max_new) * row_sum_previous) + (__expf(row_max - row_max_new) * row_sum);


            // Write output_matrix_device_pointer, sum_matrix_device_pointer, max_matrix_device_pointer to HBM
            for (int embedding_index = 0; embedding_index &lt; embedding_dimension; embedding_index++) {
                float probability_times_value = 0;  // Pij * Vj
                for (int column_index_inner = 0; column_index_inner &lt; block_size_columns; column_index_inner++) {
                    probability_times_value += score_matrix_tile[(block_size_columns * thread_index_x) + column_index_inner] * value_matrix_tile[(column_index_inner * embedding_dimension) + embedding_index]+eps;
                }
                output_matrix_device_pointer[qkv_offset + (tile_size * row_block_index) + (thread_index_x * embedding_dimension) + embedding_index] = (1 / (eps+row_sum_new)) \
                    * ((row_sum_previous * __expf(row_max_previous - row_max_new) * output_matrix_device_pointer[qkv_offset + (tile_size * row_block_index) + (thread_index_x * embedding_dimension) + embedding_index]) \
                    + (__expf(row_max - row_max_new+eps) * probability_times_value));
            }
            max_matrix_device_pointer[lm_offset + (block_size_rows * row_block_index) + thread_index_x] = row_max_new;
            sum_matrix_device_pointer[lm_offset + (block_size_rows * row_block_index) + thread_index_x] = row_sum_new;
        }
        __syncthreads();  // otherwise, thread can use the wrong key_matrix_tile, value_matrix_tile in inner loop
    }
    
}



template &lt;typename T&gt;
T* allocateAndInitializeDeviceMemory(size_t size, bool initializeToZero = false, bool initializeToNegativeInfinity = false) {
    T* device_ptr;
    cudaMalloc(&amp;device_ptr, size); // No error checking

    if (initializeToZero) {
        cudaMemset(device_ptr, 0, size); // No error checking
    } else if (initializeToNegativeInfinity) {
        float negative_infinity_host = -INFINITY;
        cudaMemset(device_ptr, *reinterpret_cast&lt;int*&gt;(&amp;negative_infinity_host), size); // No error checking
    } else {
        curandGenerator_t generator;
        curandCreateGenerator(&amp;generator, CURAND_RNG_PSEUDO_DEFAULT); // No error checking
        curandSetGeneratorOffset(generator, time(0)); // No error checking
        curandGenerateUniform(generator, reinterpret_cast&lt;float*&gt;(device_ptr), size / sizeof(T)); // No error checking
        curandDestroyGenerator(generator); // No error checking
    }

    return device_ptr;
}

template &lt;typename T&gt;
void writeMatrixToFile(T* matrix, const std::string&amp; filename, int batch_size, int num_heads, int sequence_length, int embedding_dimension) {
    std::ofstream file(filename);
    if (!file) {
        std::cerr &lt;&lt; "Could not open the file!" &lt;&lt; std::endl;
        return;
    }

    for (int b = 0; b &lt; batch_size; ++b) {
        for (int h = 0; h &lt; num_heads; ++h) {
            for (int i = 0; i &lt; sequence_length; ++i) {
                for (int j = 0; j &lt; embedding_dimension; ++j) {
                    file &lt;&lt; matrix[(b * num_heads * sequence_length * embedding_dimension) +
                                   (h * sequence_length * embedding_dimension) +
                                   (i * embedding_dimension) + j];
                    if (j &lt; embedding_dimension - 1) {
                        file &lt;&lt; ", "; // Comma separation
                    }
                }
                file &lt;&lt; std::endl; // New line for next row
            }
            file &lt;&lt; std::endl; // Extra new line for separating heads
        }
    }
    file.close();
}

template &lt;typename T&gt;
void printMatrix(T* matrix, int batch_size, int num_heads, int sequence_length, int embedding_dimension, int rowsToPrint, int colsToPrint) {
    T* host_matrix = new T[batch_size * num_heads * sequence_length * embedding_dimension];
    cudaMemcpy(host_matrix, matrix, batch_size * num_heads * sequence_length * embedding_dimension * sizeof(T), cudaMemcpyDeviceToHost);

    std::cout &lt;&lt; "Matrix:\n";
    for (int b = 0; b &lt; batch_size; ++b) {
        for (int h = 0; h &lt; num_heads; ++h) {
            for (int i = 0; i &lt; rowsToPrint; ++i) {
                for (int j = 0; j &lt; colsToPrint; ++j) {
                    std::cout &lt;&lt; host_matrix[(b * num_heads * sequence_length * embedding_dimension) +
                                            (h * sequence_length * embedding_dimension) +
                                            (i * embedding_dimension) + j] &lt;&lt; " ";
                }
                std::cout &lt;&lt; std::endl;
            }
            std::cout &lt;&lt; std::endl;
        }
    }
    delete[] host_matrix;
}
/*
void test_attention(int batch_size, int num_heads, int sequence_length, int embedding_dimension) {


    // Generate random tensors for query, key, and value (similar to CUDA initialization)
    auto query = torch::rand({batch_size, num_heads, sequence_length, embedding_dimension}, device);
    auto key = torch::rand({batch_size, num_heads, sequence_length, embedding_dimension}, device);
    auto value = torch::rand({batch_size, num_heads, sequence_length, embedding_dimension}, device);

    // Calculate the softmax scale
    float softmax_scale = 1.0f / std::sqrt(embedding_dimension);

    // Prepare output and intermediate tensors
    auto output = torch::zeros({batch_size, num_heads, sequence_length, embedding_dimension}, device);
    auto sum_matrix = torch::zeros({batch_size, num_heads, sequence_length}, device);
    auto max_matrix = torch::full({batch_size, num_heads, sequence_length}, -INFINITY, device);

    // Perform attention operation (similar to your CUDA kernel logic)
    for (int head = 0; head &lt; num_heads; ++head) {
        for (int seq_idx = 0; seq_idx &lt; sequence_length; ++seq_idx) {
            // Calculate attention scores
            auto query_vec = query.index({0, head, seq_idx, torch::indexing::Slice()});
            auto key_mat = key.index({0, head, torch::indexing::Slice(), torch::indexing::Slice()});

            auto scores = torch::matmul(query_vec.unsqueeze(0), key_mat.transpose(1, 0)) * softmax_scale;

            // Calculate softmax
            auto max_score = std::get&lt;0&gt;(scores.max(1));
            auto score_exp = torch::exp(scores - max_score.unsqueeze(1));

            // Normalize
            auto sum_score = score_exp.sum(1);
            auto attention_weights = score_exp / (sum_score.unsqueeze(1) + 1e-10);  // Avoid division by zero

            // Compute the output
            auto value_mat = value.index({0, head, torch::indexing::Slice(), torch::indexing::Slice()});
            auto weighted_value = torch::matmul(attention_weights.unsqueeze(1), value_mat).squeeze(1);
            
            output.index({0, head, seq_idx, torch::indexing::Slice()}) = weighted_value;
            max_matrix.index({0, head, seq_idx}) = max_score;
            sum_matrix.index({0, head, seq_idx}) = sum_score;
        }
    }

    // Prints to validate outputs
    std::cout &lt;&lt; "Query Tensor:\n" &lt;&lt; query &lt;&lt; "\n";
    std::cout &lt;&lt; "Key Tensor:\n" &lt;&lt; key &lt;&lt; "\n";
    std::cout &lt;&lt; "Value Tensor:\n" &lt;&lt; value &lt;&lt; "\n";
    std::cout &lt;&lt; "Output Tensor:\n" &lt;&lt; output &lt;&lt; "\n";
}
*/
int main() {
  
    const int batch_size = 1;
    const int num_heads = 1;
    const int sequence_length = 64;
    const int embedding_dimension = 64;

    
    const int block_size_columns = 32;
    const int block_size_rows = 32;

    // Derived dimensions
    const int total_columns_in_blocks = ceil((float)sequence_length / block_size_columns);
    const int total_rows_in_blocks = ceil((float)sequence_length / block_size_rows);
    const float softmax_scale = 1.0f / sqrtf(embedding_dimension);

    // Calculate sizes for memory allocation
    size_t matrix_size = batch_size * num_heads * sequence_length * embedding_dimension * sizeof(float);
    size_t vector_size = batch_size * num_heads * sequence_length * sizeof(float);


    // Device memory allocation and initialization using helper function
    float* query_matrix_device = allocateAndInitializeDeviceMemory&lt;float&gt;(matrix_size);
    float* key_matrix_device = allocateAndInitializeDeviceMemory&lt;float&gt;(matrix_size);
    float* value_matrix_device = allocateAndInitializeDeviceMemory&lt;float&gt;(matrix_size);
    float* output_matrix_device = allocateAndInitializeDeviceMemory&lt;float&gt;(matrix_size, true); // Initialize to zero
    float* sum_matrix_device = allocateAndInitializeDeviceMemory&lt;float&gt;(vector_size, false, false);  // Initialize to zero
    float* max_matrix_device = allocateAndInitializeDeviceMemory&lt;float&gt;(vector_size, false, true); // Initialize to -INFINITY
cudaMemset(sum_matrix_device, 0, vector_size);



    // Shared memory size calculation and check
    const int shared_memory_size = (4* block_size_columns * embedding_dimension * sizeof(float)) +
                                    (block_size_columns * block_size_rows * sizeof(float));
    int max_shared_memory_size;
    cudaDeviceGetAttribute(&amp;max_shared_memory_size, cudaDevAttrMaxSharedMemoryPerBlock, 0);


    // Kernel launch configuration
    dim3 grid_dim(batch_size, num_heads);
    dim3 block_dim(block_size_columns);

    forward_kernel&lt;&lt;&lt;grid_dim, block_dim, shared_memory_size&gt;&gt;&gt;(
        query_matrix_device, key_matrix_device, value_matrix_device, sequence_length,
        embedding_dimension, total_columns_in_blocks, total_rows_in_blocks, block_size_columns,
        block_size_rows, softmax_scale, sum_matrix_device, max_matrix_device, output_matrix_device);


    cudaDeviceSynchronize();  

 
    int rowsToPrint = sequence_length ;
    int colsToPrint = embedding_dimension;
    
    
    
    float* query_matrix_host = new float[batch_size * num_heads * sequence_length * embedding_dimension];
    float* key_matrix_host = new float[batch_size * num_heads * sequence_length * embedding_dimension];
    float* value_matrix_host = new float[batch_size * num_heads * sequence_length * embedding_dimension];
    float* output_matrix_host = new float[batch_size * num_heads * sequence_length * embedding_dimension];

    // Copy matrices from device to host
    cudaMemcpy(query_matrix_host, query_matrix_device, matrix_size, cudaMemcpyDeviceToHost);
    cudaMemcpy(key_matrix_host, key_matrix_device, matrix_size, cudaMemcpyDeviceToHost);
    cudaMemcpy(value_matrix_host, value_matrix_device, matrix_size, cudaMemcpyDeviceToHost);
    cudaMemcpy(output_matrix_host, output_matrix_device, matrix_size, cudaMemcpyDeviceToHost);

    // Write each matrix to its respective output file
    writeMatrixToFile(query_matrix_host, "query_output.csv", batch_size, num_heads, sequence_length, embedding_dimension);
    writeMatrixToFile(key_matrix_host, "key_output.csv", batch_size, num_heads, sequence_length, embedding_dimension);
    writeMatrixToFile(value_matrix_host, "value_output.csv", batch_size, num_heads, sequence_length, embedding_dimension);
    writeMatrixToFile(output_matrix_host, "output_output.csv", batch_size, num_heads, sequence_length, embedding_dimension);
    
    
    
    
    
    
    
    std::cout &lt;&lt; "Q:\n";
    printMatrix(query_matrix_device, batch_size, num_heads, sequence_length, embedding_dimension, rowsToPrint, colsToPrint);
    std::cout &lt;&lt; "K:\n";
    printMatrix(key_matrix_device, batch_size, num_heads, sequence_length, embedding_dimension, rowsToPrint, colsToPrint);
    std::cout &lt;&lt; "V:\n";
    printMatrix(value_matrix_device, batch_size, num_heads, sequence_length, embedding_dimension, rowsToPrint, colsToPrint);
    std::cout &lt;&lt; "O:\n";
    printMatrix(output_matrix_device, batch_size, num_heads, sequence_length, embedding_dimension, rowsToPrint, colsToPrint);

    // Free device memory
    cudaFree(query_matrix_device);
    cudaFree(key_matrix_device);
    cudaFree(value_matrix_device);
    cudaFree(output_matrix_device);
    cudaFree(sum_matrix_device);
    cudaFree(max_matrix_device);

    return 0;
}</code></pre>
    </div>

    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-markdown.min.js"></script>
</body>
</html>