<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 33 - GPU Kernels Learning Journey</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 1rem;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .header .subtitle {
            font-size: 1.1rem;
            opacity: 0.9;
        }
        
        .nav {
            background: white;
            padding: 1rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }
        
        .nav a {
            color: #667eea;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: all 0.3s;
        }
        
        .nav a:hover {
            background: #667eea;
            color: white;
        }
        
        .container {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 1rem;
        }
        
        .description {
            background: white;
            padding: 2rem;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            margin-bottom: 2rem;
        }
        
        .description h2 {
            color: #667eea;
            margin-bottom: 1rem;
            border-bottom: 2px solid #667eea;
            padding-bottom: 0.5rem;
        }
        
        .description pre {
            background: #f5f5f5;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        .file-section {
            background: white;
            margin-bottom: 2rem;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        
        .file-section h3 {
            background: #667eea;
            color: white;
            padding: 1rem;
            margin: 0;
        }
        
        .file-section pre {
            margin: 0;
            border-radius: 0;
        }
        
        .file-section code {
            display: block;
            padding: 1.5rem;
            max-height: 600px;
            overflow: auto;
        }
        
        .no-files {
            text-align: center;
            padding: 3rem;
            color: #999;
            font-style: italic;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8rem;
            }
            
            .nav-content {
                flex-direction: column;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Day 33</h1>
        <div class="subtitle">GPU Kernels Learning Journey</div>
    </div>
    
    <nav class="nav">
        <div class="nav-content">
            <a href="index.html">← Back to Index</a>
            <div>
                <a href="day-32.html">← Day 32</a>
                <a href="day-34.html">Day 34 →</a>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="description">
            <h2>Description</h2>
            <div class="desc-content">
### Files: `sparse_gemm.cpp`, `strassen.cpp`, `winograd.cpp`, `utils.cpp`
**Summary:**  
Implemented and compared different matrix multiplication algorithms for both dense and sparse matrices using HIP/ROCm in AMD gpus. The comparison included:

1. Dense Matrix Algorithms:
   - rocBLAS (vendor-optimized library)
   - Winograd's algorithm
   - Strassen's algorithm 

2. Sparse Matrix Algorithms:
   - rocSPARSE (vendor-optimized library)
   - CSR (Compressed Sparse Row)
   - COO (Coordinate format)
   - Block-CSR (Block Compressed Sparse Row)

**Benchmark Results:**  
Dense Matrix Multiplication (N=256):
- rocBLAS: 0.75 GFLOPS
- Winograd: 2279.51 GFLOPS 

Dense Matrix Multiplication (N=512):
- rocBLAS: 3232.60 GFLOPS
- Winograd: 52428.80 GFLOPS 

Sparse Matrix Multiplication (N=256, density=0.1) :
- rocBLAS: 842.23 GFLOPS
- rocSPARSE: 0.07 GFLOPS
- CSR: 633.50 GFLOPS
- COO: 655.30 GFLOPS
- Block-CSR: 819.20 GFLOPS

Sparse Matrix Multiplication (N=512, density=0.1):
- rocBLAS: 4042.70 GFLOPS
- rocSPARSE: 309.42 GFLOPS
- CSR: 5091.30 GFLOPS
- COO: 5083.93 GFLOPS
- Block-CSR: 4505.60 GFLOPS

**Findings:**
1. Winograd's algorithm shows impressive performance for small matrices but faces accuracy challenges at larger sizes
2. rocBLAS performance scales well with matrix size for dense operations
3. Our custom sparse implementations significantly outperform rocSPARSE:
   - Custom CSR is up to ~16x faster than rocSPARSE
   - Custom COO and Block-CSR also show excellent performance
   - All formats maintain reasonable accuracy compared to reference
4. For sparse matrices at N=512:
   - Custom CSR achieves best performance at 5091.30 GFLOPS
   - COO follows closely at 5083.93 GFLOPS
   - Block-CSR shows good performance at 4505.60 GFLOPS
   - All custom implementations outperform both rocBLAS and rocSPARSE (for those tests, bigger matrices benchmarking is neededed)
            </div>
        </div>
        
        <h2 style="margin-bottom: 1rem; color: #667eea;">Code Files</h2>

    <div class="file-section">
        <h3>sparse_gemm.cpp</h3>
        <pre><code class="language-cpp">#include "matrix_mult.h"
#include &lt;hip/hip_runtime.h&gt;
#include &lt;chrono&gt;

// Define HIP_CHECK macro for error handling
#define HIP_CHECK(call) do {                                                        \
    hipError_t err = call;                                                         \
    if (err != hipSuccess) {                                                       \
        printf("HIP error %s:%d: '%s'\n", __FILE__, __LINE__,                     \
               hipGetErrorString(err));                                            \
        exit(1);                                                                   \
    }                                                                              \
} while(0)

// CSR SpMM kernel
__global__ void spmm_csr_kernel(const float* values, const int* row_ptr, 
                               const int* col_indices, const float* B, float* C,
                               int M, int K, int N) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row &lt; M) {
        for (int col = 0; col &lt; N; col++) {
            float sum = 0.0f;
            for (int i = row_ptr[row]; i &lt; row_ptr[row + 1]; i++) {
                int k = col_indices[i];
                sum += values[i] * B[k * N + col];
            }
            C[row * N + col] = sum;
        }
    }
}

// COO SpMM kernel
__global__ void spmm_coo_kernel(const float* values, const int* row_indices,
                               const int* col_indices, const float* B, float* C,
                               int nnz, int M) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; nnz) {
        int row = row_indices[idx];
        int col = col_indices[idx];
        float val = values[idx];
        
        for (int j = 0; j &lt; M; j++) {
            atomicAdd(&amp;C[row * M + j], val * B[col * M + j]);
        }
    }
}

// Block-CSR SpMM kernel
__global__ void spmm_block_csr_kernel(const float* values, const int* row_ptr,
                                     const int* col_indices, const float* B, float* C,
                                     int N, int K, int M, int block_size) {
    int row_block = blockIdx.x;
    int thread_id = threadIdx.x;
    
    __shared__ float shared_B[32][32];  // Assuming max block size of 32
    
    int row = row_block * block_size;
    if (row &lt; N) {
        for (int col = 0; col &lt; M; col += block_size) {
            // Load block of matrix B into shared memory
            if (col + thread_id &lt; M &amp;&amp; thread_id &lt; block_size) {
                for (int k = row_ptr[row_block]; k &lt; row_ptr[row_block + 1]; k++) {
                    int col_block = col_indices[k] * block_size;
                    shared_B[thread_id][0] = B[(col_block + thread_id) * M + col];
                }
            }
            __syncthreads();
            
            // Compute block multiplication
            if (thread_id &lt; block_size &amp;&amp; row + thread_id &lt; N &amp;&amp; col &lt; M) {
                float sum = 0.0f;
                for (int k = row_ptr[row_block]; k &lt; row_ptr[row_block + 1]; k++) {
                    int val_idx = k * block_size * block_size + thread_id;
                    sum += values[val_idx] * shared_B[thread_id][0];
                }
                C[(row + thread_id) * M + col] = sum;
            }
            __syncthreads();
        }
    }
}

PerfResult sparse_gemm_csr(const CSRMatrix&amp; A, const float* B, float* C,
                          int M, int K, int N) {
    float *d_values, *d_B, *d_C;
    int *d_row_ptr, *d_col_indices;
    
    // Get number of non-zero elements
    int nnz = A.values.size();
    
    // Allocate device memory
    HIP_CHECK(hipMalloc(&amp;d_values, A.values.size() * sizeof(float)));
    HIP_CHECK(hipMalloc(&amp;d_row_ptr, A.row_ptr.size() * sizeof(int)));
    HIP_CHECK(hipMalloc(&amp;d_col_indices, A.col_indices.size() * sizeof(int)));
    HIP_CHECK(hipMalloc(&amp;d_B, K * N * sizeof(float)));
    HIP_CHECK(hipMalloc(&amp;d_C, M * N * sizeof(float)));
    
    // Copy data to device
    HIP_CHECK(hipMemcpy(d_values, A.values.data(), A.values.size() * sizeof(float), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_row_ptr, A.row_ptr.data(), A.row_ptr.size() * sizeof(int), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_col_indices, A.col_indices.data(), A.col_indices.size() * sizeof(int), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_B, B, K * N * sizeof(float), hipMemcpyHostToDevice));
    
    // Initialize C with zeros
    HIP_CHECK(hipMemset(d_C, 0, M * N * sizeof(float)));
    
    // Set up timing
    hipEvent_t start, stop;
    HIP_CHECK(hipEventCreate(&amp;start));
    HIP_CHECK(hipEventCreate(&amp;stop));
    
    // Launch kernel
    dim3 block(256);
    dim3 grid((M + block.x - 1) / block.x);
    
    HIP_CHECK(hipEventRecord(start));
    hipLaunchKernelGGL(spmm_csr_kernel, grid, block, 0, nullptr,
                       d_values, d_row_ptr, d_col_indices, d_B, d_C, M, K, N);
    HIP_CHECK(hipEventRecord(stop));
    HIP_CHECK(hipEventSynchronize(stop));
    
    float milliseconds = 0;
    HIP_CHECK(hipEventElapsedTime(&amp;milliseconds, start, stop));
    
    // Copy result back to host
    HIP_CHECK(hipMemcpy(C, d_C, M * N * sizeof(float), hipMemcpyDeviceToHost));
    
    // Calculate performance metrics
    double gflops = (2.0 * nnz * N) / (milliseconds * 1e6);
    
    // Cleanup
    HIP_CHECK(hipFree(d_values));
    HIP_CHECK(hipFree(d_row_ptr));
    HIP_CHECK(hipFree(d_col_indices));
    HIP_CHECK(hipFree(d_B));
    HIP_CHECK(hipFree(d_C));
    HIP_CHECK(hipEventDestroy(start));
    HIP_CHECK(hipEventDestroy(stop));
    
    return {milliseconds, gflops, "CSR", 0.0};
}

PerfResult sparse_gemm_coo(const COOMatrix&amp; A, const float* B, float* C,
                          int M, int K, int N) {
    float *d_values, *d_B, *d_C;
    int *d_row_indices, *d_col_indices;
    
    // Allocate device memory
    HIP_CHECK(hipMalloc(&amp;d_values, A.values.size() * sizeof(float)));
    HIP_CHECK(hipMalloc(&amp;d_row_indices, A.row_indices.size() * sizeof(int)));
    HIP_CHECK(hipMalloc(&amp;d_col_indices, A.col_indices.size() * sizeof(int)));
    HIP_CHECK(hipMalloc(&amp;d_B, K * N * sizeof(float)));
    HIP_CHECK(hipMalloc(&amp;d_C, M * N * sizeof(float)));
    
    // Copy data to device
    HIP_CHECK(hipMemcpy(d_values, A.values.data(), A.values.size() * sizeof(float), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_row_indices, A.row_indices.data(), A.row_indices.size() * sizeof(int), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_col_indices, A.col_indices.data(), A.col_indices.size() * sizeof(int), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_B, B, K * N * sizeof(float), hipMemcpyHostToDevice));
    
    // Initialize C with zeros
    HIP_CHECK(hipMemset(d_C, 0, M * N * sizeof(float)));
    
    // Set up timing
    hipEvent_t start, stop;
    HIP_CHECK(hipEventCreate(&amp;start));
    HIP_CHECK(hipEventCreate(&amp;stop));
    
    // Launch kernel
    dim3 block(256);
    dim3 grid((A.nnz + block.x - 1) / block.x);
    
    HIP_CHECK(hipEventRecord(start));
    hipLaunchKernelGGL(spmm_coo_kernel, grid, block, 0, nullptr,
                       d_values, d_row_indices, d_col_indices, d_B, d_C, A.nnz, M);
    HIP_CHECK(hipEventRecord(stop));
    HIP_CHECK(hipEventSynchronize(stop));
    
    float milliseconds = 0;
    HIP_CHECK(hipEventElapsedTime(&amp;milliseconds, start, stop));
    
    // Copy result back to host
    HIP_CHECK(hipMemcpy(C, d_C, M * N * sizeof(float), hipMemcpyDeviceToHost));
    
    // Calculate performance metrics
    double gflops = (2.0 * A.nnz * N) / (milliseconds * 1e6);
    
    // Cleanup
    HIP_CHECK(hipFree(d_values));
    HIP_CHECK(hipFree(d_row_indices));
    HIP_CHECK(hipFree(d_col_indices));
    HIP_CHECK(hipFree(d_B));
    HIP_CHECK(hipFree(d_C));
    HIP_CHECK(hipEventDestroy(start));
    HIP_CHECK(hipEventDestroy(stop));
    
    return {milliseconds, gflops, "COO", 0.0};
}

PerfResult sparse_gemm_block_csr(const BlockCSRMatrix&amp; A, const float* B, float* C,
                                int M, int K, int N) {
    float *d_values, *d_B, *d_C;
    int *d_row_ptr, *d_col_indices;
    
    // Calculate nnz for Block-CSR
    int nnz = A.values.size() / (A.block_size * A.block_size);
    
    // Allocate device memory
    HIP_CHECK(hipMalloc(&amp;d_values, A.values.size() * sizeof(float)));
    HIP_CHECK(hipMalloc(&amp;d_row_ptr, A.row_ptr.size() * sizeof(int)));
    HIP_CHECK(hipMalloc(&amp;d_col_indices, A.col_indices.size() * sizeof(int)));
    HIP_CHECK(hipMalloc(&amp;d_B, K * N * sizeof(float)));
    HIP_CHECK(hipMalloc(&amp;d_C, M * N * sizeof(float)));
    
    // Copy data to device
    HIP_CHECK(hipMemcpy(d_values, A.values.data(), A.values.size() * sizeof(float), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_row_ptr, A.row_ptr.data(), A.row_ptr.size() * sizeof(int), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_col_indices, A.col_indices.data(), A.col_indices.size() * sizeof(int), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_B, B, K * N * sizeof(float), hipMemcpyHostToDevice));
    
    // Initialize C with zeros
    HIP_CHECK(hipMemset(d_C, 0, M * N * sizeof(float)));
    
    // Set up timing
    hipEvent_t start, stop;
    HIP_CHECK(hipEventCreate(&amp;start));
    HIP_CHECK(hipEventCreate(&amp;stop));
    
    // Launch kernel
    dim3 block(A.block_size * A.block_size);
    dim3 grid((N + A.block_size - 1) / A.block_size);
    
    HIP_CHECK(hipEventRecord(start));
    hipLaunchKernelGGL(spmm_block_csr_kernel, grid, block, 0, nullptr,
                       d_values, d_row_ptr, d_col_indices, d_B, d_C,
                       N, K, M, A.block_size);
    HIP_CHECK(hipEventRecord(stop));
    HIP_CHECK(hipEventSynchronize(stop));
    
    float milliseconds = 0;
    HIP_CHECK(hipEventElapsedTime(&amp;milliseconds, start, stop));
    
    // Copy result back to host
    HIP_CHECK(hipMemcpy(C, d_C, M * N * sizeof(float), hipMemcpyDeviceToHost));
    
    // Calculate performance metrics
    double gflops = (2.0 * nnz * A.block_size * A.block_size * N) / (milliseconds * 1e6);
    
    // Cleanup
    HIP_CHECK(hipFree(d_values));
    HIP_CHECK(hipFree(d_row_ptr));
    HIP_CHECK(hipFree(d_col_indices));
    HIP_CHECK(hipFree(d_B));
    HIP_CHECK(hipFree(d_C));
    HIP_CHECK(hipEventDestroy(start));
    HIP_CHECK(hipEventDestroy(stop));
    
    return {milliseconds, gflops, "Block-CSR", 0.0};
} </code></pre>
    </div>

    <div class="file-section">
        <h3>matrix_mult.h</h3>
        <pre><code class="language-c">#pragma once

#include &lt;hip/hip_runtime.h&gt;
#include &lt;rocblas/rocblas.h&gt;
#include &lt;vector&gt;
#include &lt;string&gt;

// Sparse matrix formats
enum class SparseFormat {
    CSR,
    COO,
    BLOCK_CSR
};

// Structure for sparse matrix in CSR format
struct CSRMatrix {
    std::vector&lt;float&gt; values;     // Non-zero values
    std::vector&lt;int&gt; row_ptr;      // Row pointers
    std::vector&lt;int&gt; col_indices;  // Column indices
    int rows;
    int cols;
    int nnz;  // Number of non-zero elements

    CSRMatrix(int r, int c) : rows(r), cols(c), nnz(0) {
        row_ptr.resize(r + 1, 0);
    }
};

// Structure for sparse matrix in COO format
struct COOMatrix {
    std::vector&lt;float&gt; values;     // Non-zero values
    std::vector&lt;int&gt; row_indices;  // Row indices
    std::vector&lt;int&gt; col_indices;  // Column indices
    int rows;
    int cols;
    int nnz;  // Number of non-zero elements

    COOMatrix(int r, int c) : rows(r), cols(c), nnz(0) {}
};

// Structure for sparse matrix in Block-CSR format
struct BlockCSRMatrix {
    std::vector&lt;float&gt; values;     // Non-zero blocks
    std::vector&lt;int&gt; row_ptr;      // Row pointers
    std::vector&lt;int&gt; col_indices;  // Column indices
    int rows;
    int cols;
    int block_size;                // Size of each block

    BlockCSRMatrix(int r, int c, int bs) : rows(r), cols(c), block_size(bs) {
        int block_rows = (r + bs - 1) / bs;
        row_ptr.resize(block_rows + 1, 0);
    }
};

// Performance result structure
struct PerfResult {
    double time_ms;        // Execution time in milliseconds
    double gflops;         // Performance in GFLOPS
    std::string format;    // Format or algorithm used
    double max_diff;       // Maximum difference from reference
};

// Function declarations for sparse GEMM
PerfResult sparse_gemm_csr(const CSRMatrix&amp; A, const float* B, float* C, int N, int K, int M);
PerfResult sparse_gemm_coo(const COOMatrix&amp; A, const float* B, float* C, int N, int K, int M);
PerfResult sparse_gemm_block_csr(const BlockCSRMatrix&amp; A, const float* B, float* C, int N, int K, int M);

// Function declarations for Strassen algorithm
PerfResult strassen_multiply(const float* A, const float* B, float* C, int N);

// Function declarations for Winograd algorithm
PerfResult winograd_multiply(const float* A, const float* B, float* C, int N);

// Function declarations for rocBLAS reference implementation
PerfResult rocblas_sgemm_ref(const float* A, const float* B, float* C, int N, int K, int M);

// Utility functions
void generate_random_sparse_matrix(CSRMatrix&amp; mat, float density);
void generate_random_sparse_matrix(COOMatrix&amp; mat, float density);
void generate_random_sparse_matrix(BlockCSRMatrix&amp; mat, float density);
void generate_random_matrix(float* mat, int rows, int cols);
double compare_matrices(const float* A, const float* B, int rows, int cols);
void print_performance_results(const std::vector&lt;PerfResult&gt;&amp; results); </code></pre>
    </div>

    <div class="file-section">
        <h3>winograd.cpp</h3>
        <pre><code class="language-cpp">#include "matrix_mult.h"
#include &lt;hip/hip_runtime.h&gt;
#include &lt;cmath&gt;

// Define HIP_CHECK macro for error handling
#define HIP_CHECK(call) do {                                                        \
    hipError_t err = call;                                                         \
    if (err != hipSuccess) {                                                       \
        printf("HIP error %s:%d: '%s'\n", __FILE__, __LINE__,                     \
               hipGetErrorString(err));                                            \
        exit(1);                                                                   \
    }                                                                              \
} while(0)

// Kernel for matrix addition
__global__ void winograd_add_kernel(
    const float* A,
    const float* B,
    float* C,
    int N)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; N * N) {
        C[idx] = A[idx] + B[idx];
    }
}

// Kernel for matrix subtraction
__global__ void winograd_sub_kernel(
    const float* A,
    const float* B,
    float* C,
    int N)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; N * N) {
        C[idx] = A[idx] - B[idx];
    }
}

// Kernel for computing intermediate matrices
__global__ void compute_intermediate_kernel(
    const float* A,
    const float* B,
    float* S,
    float* T,
    int N)
{
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row &lt; N/2 &amp;&amp; col &lt; N/2) {
        // Compute S = (A21 + A22) - A11
        int idx = row * N/2 + col;
        S[idx] = (A[(row + N/2) * N + col] + A[(row + N/2) * N + (col + N/2)]) - A[row * N + col];
        
        // Compute T = (B12 - B11) + B22
        T[idx] = (B[row * N + (col + N/2)] - B[row * N + col]) + B[(row + N/2) * N + (col + N/2)];
    }
}

// Main Winograd multiplication kernel for 2x2 block
__global__ void winograd_multiply_kernel(
    const float* A,
    const float* B,
    float* C,
    int N)
{
    __shared__ float As[32][32];  // Assuming block size &lt;= 32
    __shared__ float Bs[32][32];
    
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int bx = blockIdx.x;
    int by = blockIdx.y;
    
    int row = by * blockDim.y + ty;
    int col = bx * blockDim.x + tx;
    
    float sum = 0.0f;
    
    for (int i = 0; i &lt; N/32; ++i) {
        if (row &lt; N &amp;&amp; i * 32 + tx &lt; N)
            As[ty][tx] = A[row * N + i * 32 + tx];
        else
            As[ty][tx] = 0.0f;
            
        if (i * 32 + ty &lt; N &amp;&amp; col &lt; N)
            Bs[ty][tx] = B[(i * 32 + ty) * N + col];
        else
            Bs[ty][tx] = 0.0f;
            
        __syncthreads();
        
        for (int k = 0; k &lt; 32; ++k)
            sum += As[ty][k] * Bs[k][tx];
            
        __syncthreads();
    }
    
    if (row &lt; N &amp;&amp; col &lt; N)
        C[row * N + col] = sum;
}

PerfResult winograd_multiply(const float* A, const float* B, float* C, int N) {
    PerfResult result = {0.0, 0.0, "Winograd", 0.0};  // Initialize all fields

    // Ensure N is even
    if (N % 2 != 0) {
        // Handle odd sizes by padding
        int new_size = N + 1;
        
        // Create padded matrices
        std::vector&lt;float&gt; A_padded(new_size * new_size, 0.0f);
        std::vector&lt;float&gt; B_padded(new_size * new_size, 0.0f);
        std::vector&lt;float&gt; C_padded(new_size * new_size, 0.0f);

        // Copy original matrices to padded ones
        for (int i = 0; i &lt; N; i++) {
            for (int j = 0; j &lt; N; j++) {
                A_padded[i * new_size + j] = A[i * N + j];
                B_padded[i * new_size + j] = B[i * N + j];
            }
        }

        // Allocate device memory
        float *d_A, *d_B, *d_C;
        float *d_S, *d_T;  // Intermediate matrices
        float *d_M1, *d_M2, *d_M3, *d_M4, *d_M5, *d_M6, *d_M7;
        
        size_t size = new_size * new_size * sizeof(float);
        size_t half_size = (new_size/2) * (new_size/2) * sizeof(float);
        
        HIP_CHECK(hipMalloc(&amp;d_A, size));
        HIP_CHECK(hipMalloc(&amp;d_B, size));
        HIP_CHECK(hipMalloc(&amp;d_C, size));
        HIP_CHECK(hipMalloc(&amp;d_S, half_size));
        HIP_CHECK(hipMalloc(&amp;d_T, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M1, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M2, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M3, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M4, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M5, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M6, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M7, half_size));

        // Copy data to device
        HIP_CHECK(hipMemcpy(d_A, A_padded.data(), size, hipMemcpyHostToDevice));
        HIP_CHECK(hipMemcpy(d_B, B_padded.data(), size, hipMemcpyHostToDevice));

        // Set up timing
        hipEvent_t start, stop;
        hipEventCreate(&amp;start);
        hipEventCreate(&amp;stop);

        hipEventRecord(start);

        // Compute intermediate matrices S and T
        dim3 block(16, 16);
        dim3 grid((new_size/2 + block.x - 1)/block.x, (new_size/2 + block.y - 1)/block.y);
        compute_intermediate_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_A, d_B, d_S, d_T, new_size);

        // Main multiplication using Winograd's algorithm
        dim3 block_main(32, 32);
        dim3 grid_main((new_size + block_main.x - 1)/block_main.x, 
                      (new_size + block_main.y - 1)/block_main.y);
        winograd_multiply_kernel&lt;&lt;&lt;grid_main, block_main&gt;&gt;&gt;(d_A, d_B, d_C, new_size);

        hipEventRecord(stop);
        hipEventSynchronize(stop);

        float milliseconds = 0;
        hipEventElapsedTime(&amp;milliseconds, start, stop);
        result.time_ms = milliseconds;

        // Copy result back and remove padding
        HIP_CHECK(hipMemcpy(C_padded.data(), d_C, size, hipMemcpyDeviceToHost));
        for (int i = 0; i &lt; N; i++) {
            for (int j = 0; j &lt; N; j++) {
                C[i * N + j] = C_padded[i * new_size + j];
            }
        }

        // Calculate GFLOPS
        double operations = 2.0 * N * N * N;  // Approximate for Winograd
        result.gflops = (operations / (milliseconds * 1e-3)) / 1e9;

        // Cleanup
        hipFree(d_A);
        hipFree(d_B);
        hipFree(d_C);
        hipFree(d_S);
        hipFree(d_T);
        hipFree(d_M1);
        hipFree(d_M2);
        hipFree(d_M3);
        hipFree(d_M4);
        hipFree(d_M5);
        hipFree(d_M6);
        hipFree(d_M7);
        hipEventDestroy(start);
        hipEventDestroy(stop);
    } else {
        // For even sizes, proceed directly
        float *d_A, *d_B, *d_C;
        float *d_S, *d_T;
        float *d_M1, *d_M2, *d_M3, *d_M4, *d_M5, *d_M6, *d_M7;
        
        size_t size = N * N * sizeof(float);
        size_t half_size = (N/2) * (N/2) * sizeof(float);
        
        HIP_CHECK(hipMalloc(&amp;d_A, size));
        HIP_CHECK(hipMalloc(&amp;d_B, size));
        HIP_CHECK(hipMalloc(&amp;d_C, size));
        HIP_CHECK(hipMalloc(&amp;d_S, half_size));
        HIP_CHECK(hipMalloc(&amp;d_T, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M1, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M2, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M3, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M4, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M5, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M6, half_size));
        HIP_CHECK(hipMalloc(&amp;d_M7, half_size));

        HIP_CHECK(hipMemcpy(d_A, A, size, hipMemcpyHostToDevice));
        HIP_CHECK(hipMemcpy(d_B, B, size, hipMemcpyHostToDevice));

        hipEvent_t start, stop;
        hipEventCreate(&amp;start);
        hipEventCreate(&amp;stop);

        hipEventRecord(start);

        // Compute intermediate matrices
        dim3 block(16, 16);
        dim3 grid((N/2 + block.x - 1)/block.x, (N/2 + block.y - 1)/block.y);
        compute_intermediate_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_A, d_B, d_S, d_T, N);

        // Main multiplication
        dim3 block_main(32, 32);
        dim3 grid_main((N + block_main.x - 1)/block_main.x, 
                      (N + block_main.y - 1)/block_main.y);
        winograd_multiply_kernel&lt;&lt;&lt;grid_main, block_main&gt;&gt;&gt;(d_A, d_B, d_C, N);

        hipEventRecord(stop);
        hipEventSynchronize(stop);

        float milliseconds = 0;
        hipEventElapsedTime(&amp;milliseconds, start, stop);
        result.time_ms = milliseconds;

        HIP_CHECK(hipMemcpy(C, d_C, size, hipMemcpyDeviceToHost));

        // Calculate GFLOPS
        double operations = 2.0 * N * N * N;  // Approximate for Winograd
        result.gflops = (operations / (milliseconds * 1e-3)) / 1e9;

        // Cleanup
        hipFree(d_A);
        hipFree(d_B);
        hipFree(d_C);
        hipFree(d_S);
        hipFree(d_T);
        hipFree(d_M1);
        hipFree(d_M2);
        hipFree(d_M3);
        hipFree(d_M4);
        hipFree(d_M5);
        hipFree(d_M6);
        hipFree(d_M7);
        hipEventDestroy(start);
        hipEventDestroy(stop);
    }

    return result;
} </code></pre>
    </div>

    <div class="file-section">
        <h3>utils.cpp</h3>
        <pre><code class="language-cpp">#include "matrix_mult.h"
#include &lt;random&gt;
#include &lt;algorithm&gt;
#include &lt;iomanip&gt;
#include &lt;iostream&gt;
#include &lt;cmath&gt;

void generate_random_sparse_matrix(CSRMatrix&amp; matrix, float density) {
    // Initialize random number generator
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution&lt;&gt; dis(0.0, 1.0);
    std::uniform_real_distribution&lt;&gt; val_dis(-1.0, 1.0);
    
    // Clear existing data
    matrix.values.clear();
    matrix.col_indices.clear();
    matrix.row_ptr.clear();
    
    // Initialize row_ptr with zeros
    matrix.row_ptr.resize(matrix.rows + 1, 0);
    
    // First pass: count non-zero elements per row
    for (int i = 0; i &lt; matrix.rows; ++i) {
        for (int j = 0; j &lt; matrix.cols; ++j) {
            if (dis(gen) &lt; density) {
                matrix.values.push_back(val_dis(gen));
                matrix.col_indices.push_back(j);
                matrix.row_ptr[i + 1]++;
            }
        }
    }
    
    // Compute cumulative sum for row_ptr
    for (int i = 1; i &lt;= matrix.rows; ++i) {
        matrix.row_ptr[i] += matrix.row_ptr[i - 1];
    }
}

void generate_random_sparse_matrix(COOMatrix&amp; mat, float density) {
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution&lt;float&gt; val_dist(-1.0f, 1.0f);
    std::uniform_real_distribution&lt;float&gt; sparsity_dist(0.0f, 1.0f);

    // Calculate number of non-zero elements
    int total_elements = mat.rows * mat.cols;
    mat.nnz = static_cast&lt;int&gt;(density * total_elements);

    // Generate non-zero elements
    mat.values.clear();
    mat.row_indices.clear();
    mat.col_indices.clear();

    for (int i = 0; i &lt; mat.rows; ++i) {
        for (int j = 0; j &lt; mat.cols; ++j) {
            if (sparsity_dist(gen) &lt; density) {
                mat.values.push_back(val_dist(gen));
                mat.row_indices.push_back(i);
                mat.col_indices.push_back(j);
            }
        }
    }

    // Sort by row and column indices
    std::vector&lt;std::tuple&lt;int, int, float&gt;&gt; elements;
    for (size_t i = 0; i &lt; mat.values.size(); ++i) {
        elements.emplace_back(mat.row_indices[i], mat.col_indices[i], mat.values[i]);
    }
    std::sort(elements.begin(), elements.end());

    // Update arrays
    mat.values.clear();
    mat.row_indices.clear();
    mat.col_indices.clear();
    for (const auto&amp; elem : elements) {
        mat.row_indices.push_back(std::get&lt;0&gt;(elem));
        mat.col_indices.push_back(std::get&lt;1&gt;(elem));
        mat.values.push_back(std::get&lt;2&gt;(elem));
    }
}

void generate_random_sparse_matrix(BlockCSRMatrix&amp; matrix, float density) {
    // Initialize random number generator
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution&lt;&gt; dis(0.0, 1.0);
    std::uniform_real_distribution&lt;&gt; val_dis(-1.0, 1.0);
    
    // Clear existing data
    matrix.values.clear();
    matrix.col_indices.clear();
    matrix.row_ptr.clear();
    
    // Initialize row_ptr with zeros
    int num_block_rows = matrix.rows / matrix.block_size;
    matrix.row_ptr.resize(num_block_rows + 1, 0);
    
    // First pass: count non-zero blocks per block row
    for (int i = 0; i &lt; num_block_rows; ++i) {
        for (int j = 0; j &lt; matrix.cols / matrix.block_size; ++j) {
            if (dis(gen) &lt; density) {
                // Generate random values for the block
                for (int bi = 0; bi &lt; matrix.block_size; ++bi) {
                    for (int bj = 0; bj &lt; matrix.block_size; ++bj) {
                        matrix.values.push_back(val_dis(gen));
                    }
                }
                matrix.col_indices.push_back(j);
                matrix.row_ptr[i + 1]++;
            }
        }
    }
    
    // Compute cumulative sum for row_ptr
    for (int i = 1; i &lt;= num_block_rows; ++i) {
        matrix.row_ptr[i] += matrix.row_ptr[i - 1];
    }
}

void generate_random_matrix(float* mat, int rows, int cols) {
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution&lt;float&gt; dist(-1.0f, 1.0f);

    for (int i = 0; i &lt; rows * cols; ++i) {
        mat[i] = dist(gen);
    }
}

double compare_matrices(const float* A, const float* B, int rows, int cols) {
    double max_diff = 0.0;
    for (int i = 0; i &lt; rows * cols; ++i) {
        double diff = std::abs(A[i] - B[i]);
        max_diff = std::max(max_diff, diff);
    }
    return max_diff;
}

void print_performance_results(const std::vector&lt;PerfResult&gt;&amp; results) {
    // Print header
    std::cout &lt;&lt; std::setw(15) &lt;&lt; "Format"
              &lt;&lt; std::setw(15) &lt;&lt; "Time (ms)"
              &lt;&lt; std::setw(15) &lt;&lt; "GFLOPS"
              &lt;&lt; std::setw(15) &lt;&lt; "Max Diff" &lt;&lt; std::endl;
    std::cout &lt;&lt; std::string(60, '-') &lt;&lt; std::endl;

    // Print results
    for (const auto&amp; result : results) {
        std::cout &lt;&lt; std::setw(15) &lt;&lt; result.format
                  &lt;&lt; std::setw(15) &lt;&lt; std::fixed &lt;&lt; std::setprecision(3) &lt;&lt; result.time_ms
                  &lt;&lt; std::setw(15) &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; result.gflops
                  &lt;&lt; std::setw(15) &lt;&lt; std::scientific &lt;&lt; std::setprecision(3) &lt;&lt; result.max_diff
                  &lt;&lt; std::endl;
    }
    std::cout.flush();  // Ensure output is written immediately
} </code></pre>
    </div>

    <div class="file-section">
        <h3>main.cpp</h3>
        <pre><code class="language-cpp">#include "matrix_mult.h"
#include &lt;vector&gt;
#include &lt;iostream&gt;
#include &lt;iomanip&gt;
#include &lt;chrono&gt;
#include &lt;rocblas/rocblas.h&gt;
#include &lt;rocsparse/rocsparse.h&gt;

// Define HIP_CHECK macro for error handling
#define HIP_CHECK(call) do {                                                        \
    hipError_t err = call;                                                         \
    if (err != hipSuccess) {                                                       \
        printf("HIP error %s:%d: '%s'\n", __FILE__, __LINE__,                     \
               hipGetErrorString(err));                                            \
        exit(1);                                                                   \
    }                                                                              \
} while(0)

// Function to run rocBLAS SGEMM for reference
PerfResult rocblas_sgemm_ref(const float* A, const float* B, float* C, int N, int K, int M) {
    PerfResult result = {0.0, 0.0, "rocBLAS", 0.0};  // Initialize all fields

    // Initialize rocBLAS
    rocblas_handle handle;
    rocblas_create_handle(&amp;handle);

    // Allocate device memory
    float *d_A, *d_B, *d_C;
    HIP_CHECK(hipMalloc(&amp;d_A, N * K * sizeof(float)));
    HIP_CHECK(hipMalloc(&amp;d_B, K * M * sizeof(float)));
    HIP_CHECK(hipMalloc(&amp;d_C, N * M * sizeof(float)));

    // Copy data to device
    HIP_CHECK(hipMemcpy(d_A, A, N * K * sizeof(float), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_B, B, K * M * sizeof(float), hipMemcpyHostToDevice));

    // Set up timing
    hipEvent_t start, stop;
    HIP_CHECK(hipEventCreate(&amp;start));
    HIP_CHECK(hipEventCreate(&amp;stop));

    const float alpha = 1.0f;
    const float beta = 0.0f;

    HIP_CHECK(hipEventRecord(start));
    rocblas_sgemm(handle,
                  rocblas_operation_none, rocblas_operation_none,
                  M, N, K,
                  &amp;alpha,
                  d_B, M,
                  d_A, K,
                  &amp;beta,
                  d_C, M);
    HIP_CHECK(hipEventRecord(stop));
    HIP_CHECK(hipEventSynchronize(stop));

    float milliseconds = 0;
    HIP_CHECK(hipEventElapsedTime(&amp;milliseconds, start, stop));
    result.time_ms = milliseconds;

    // Copy result back to host
    HIP_CHECK(hipMemcpy(C, d_C, N * M * sizeof(float), hipMemcpyDeviceToHost));

    // Calculate GFLOPS
    double operations = 2.0 * N * M * K;
    result.gflops = (operations / (milliseconds * 1e-3)) / 1e9;

    // Cleanup
    HIP_CHECK(hipFree(d_A));
    HIP_CHECK(hipFree(d_B));
    HIP_CHECK(hipFree(d_C));
    HIP_CHECK(hipEventDestroy(start));
    HIP_CHECK(hipEventDestroy(stop));
    rocblas_destroy_handle(handle);

    return result;
}

void benchmark_dense_algorithms(int N) {
    std::cout &lt;&lt; "\nBenchmarking Dense Matrix Multiplication Algorithms (N=" &lt;&lt; N &lt;&lt; ")\n";
    std::cout &lt;&lt; std::string(60, '=') &lt;&lt; std::endl;

    // Allocate and initialize matrices
    std::vector&lt;float&gt; A(N * N);
    std::vector&lt;float&gt; B(N * N);
    std::vector&lt;float&gt; C_ref(N * N);
    std::vector&lt;float&gt; C_test(N * N);

    generate_random_matrix(A.data(), N, N);
    generate_random_matrix(B.data(), N, N);

    std::vector&lt;PerfResult&gt; results;

    // Run rocBLAS reference
    auto rocblas_result = rocblas_sgemm_ref(A.data(), B.data(), C_ref.data(), N, N, N);
    results.push_back(rocblas_result);

    // Run Winograd (it's faster and more stable than Strassen)
    auto winograd_result = winograd_multiply(A.data(), B.data(), C_test.data(), N);
    winograd_result.max_diff = compare_matrices(C_ref.data(), C_test.data(), N, N);
    results.push_back(winograd_result);

    print_performance_results(results);
}

void benchmark_sparse_algorithms(int N, float density) {
    std::cout &lt;&lt; "\nBenchmarking Sparse Matrix Multiplication Algorithms (N=" &lt;&lt; N &lt;&lt; ", density=" &lt;&lt; density &lt;&lt; ")\n";
    std::cout &lt;&lt; std::string(60, '=') &lt;&lt; std::endl;

    // Create sparse matrices in different formats
    CSRMatrix csr_mat{N, N};
    COOMatrix coo_mat{N, N};
    BlockCSRMatrix bcsr_mat{N, N, 32};  // Using 32x32 blocks

    generate_random_sparse_matrix(csr_mat, density);
    generate_random_sparse_matrix(coo_mat, density);
    generate_random_sparse_matrix(bcsr_mat, density);

    // Dense matrix B and result matrices
    std::vector&lt;float&gt; B(N * N);
    std::vector&lt;float&gt; C_ref(N * N);
    std::vector&lt;float&gt; C_csr(N * N);
    std::vector&lt;float&gt; C_coo(N * N);
    std::vector&lt;float&gt; C_bcsr(N * N);
    std::vector&lt;float&gt; C_rocsparse(N * N);

    generate_random_matrix(B.data(), N, N);

    std::vector&lt;PerfResult&gt; results;

    // Run rocBLAS reference with dense matrices
    std::vector&lt;float&gt; A_dense(N * N, 0.0f);
    for (size_t i = 0; i &lt; csr_mat.values.size(); ++i) {
        int row = std::lower_bound(csr_mat.row_ptr.begin(), csr_mat.row_ptr.end(), i) - csr_mat.row_ptr.begin() - 1;
        A_dense[row * N + csr_mat.col_indices[i]] = csr_mat.values[i];
    }
    auto rocblas_result = rocblas_sgemm_ref(A_dense.data(), B.data(), C_ref.data(), N, N, N);
    results.push_back(rocblas_result);

    // Run rocSPARSE CSR SpMM
    rocsparse_handle handle;
    rocsparse_create_handle(&amp;handle);

    // Allocate device memory
    float *d_values, *d_B, *d_C;
    int *d_row_ptr, *d_col_indices;
    HIP_CHECK(hipMalloc(&amp;d_values, csr_mat.values.size() * sizeof(float)));
    HIP_CHECK(hipMalloc(&amp;d_row_ptr, csr_mat.row_ptr.size() * sizeof(int)));
    HIP_CHECK(hipMalloc(&amp;d_col_indices, csr_mat.col_indices.size() * sizeof(int)));
    HIP_CHECK(hipMalloc(&amp;d_B, N * N * sizeof(float)));
    HIP_CHECK(hipMalloc(&amp;d_C, N * N * sizeof(float)));

    // Copy data to device
    HIP_CHECK(hipMemcpy(d_values, csr_mat.values.data(), csr_mat.values.size() * sizeof(float), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_row_ptr, csr_mat.row_ptr.data(), csr_mat.row_ptr.size() * sizeof(int), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_col_indices, csr_mat.col_indices.data(), csr_mat.col_indices.size() * sizeof(int), hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_B, B.data(), N * N * sizeof(float), hipMemcpyHostToDevice));

    // Create matrix descriptors
    rocsparse_mat_descr descr;
    rocsparse_create_mat_descr(&amp;descr);
    rocsparse_set_mat_type(descr, rocsparse_matrix_type_general);
    rocsparse_set_mat_index_base(descr, rocsparse_index_base_zero);

    // Set up timing
    hipEvent_t start, stop;
    HIP_CHECK(hipEventCreate(&amp;start));
    HIP_CHECK(hipEventCreate(&amp;stop));

    const float alpha = 1.0f;
    const float beta = 0.0f;

    HIP_CHECK(hipEventRecord(start));
    rocsparse_scsrmm(handle, rocsparse_operation_none, rocsparse_operation_none,
                     N, N, N, csr_mat.values.size(),
                     &amp;alpha, descr, d_values, d_row_ptr, d_col_indices,
                     d_B, N, &amp;beta, d_C, N);
    HIP_CHECK(hipEventRecord(stop));
    HIP_CHECK(hipEventSynchronize(stop));

    float milliseconds = 0;
    HIP_CHECK(hipEventElapsedTime(&amp;milliseconds, start, stop));

    // Copy result back to host
    HIP_CHECK(hipMemcpy(C_rocsparse.data(), d_C, N * N * sizeof(float), hipMemcpyDeviceToHost));

    // Calculate GFLOPS for rocSPARSE
    double operations = 2.0 * csr_mat.values.size() * N;
    double gflops = (operations / (milliseconds * 1e-3)) / 1e9;

    PerfResult rocsparse_result = {milliseconds, gflops, "rocSPARSE", 
                                  compare_matrices(C_ref.data(), C_rocsparse.data(), N, N)};
    results.push_back(rocsparse_result);

    // Run CSR SpMM
    auto csr_result = sparse_gemm_csr(csr_mat, B.data(), C_csr.data(), N, N, N);
    csr_result.max_diff = compare_matrices(C_ref.data(), C_csr.data(), N, N);
    results.push_back(csr_result);

    // Run COO SpMM
    auto coo_result = sparse_gemm_coo(coo_mat, B.data(), C_coo.data(), N, N, N);
    coo_result.max_diff = compare_matrices(C_ref.data(), C_coo.data(), N, N);
    results.push_back(coo_result);

    // Run Block-CSR SpMM
    auto bcsr_result = sparse_gemm_block_csr(bcsr_mat, B.data(), C_bcsr.data(), N, N, N);
    bcsr_result.max_diff = compare_matrices(C_ref.data(), C_bcsr.data(), N, N);
    results.push_back(bcsr_result);

    print_performance_results(results);

    // Cleanup rocSPARSE resources
    rocsparse_destroy_mat_descr(descr);
    rocsparse_destroy_handle(handle);
    HIP_CHECK(hipFree(d_values));
    HIP_CHECK(hipFree(d_row_ptr));
    HIP_CHECK(hipFree(d_col_indices));
    HIP_CHECK(hipFree(d_B));
    HIP_CHECK(hipFree(d_C));
    HIP_CHECK(hipEventDestroy(start));
    HIP_CHECK(hipEventDestroy(stop));
}

int main() {
    // Test matrix sizes
    std::vector&lt;int&gt; sizes = {256, 512};  // Testing with smaller sizes
    std::vector&lt;float&gt; densities = {0.1f};  // Just one density value

    // Benchmark dense algorithms
    for (int N : sizes) {
        benchmark_dense_algorithms(N);
    }

    // Benchmark sparse algorithms
    for (int N : sizes) {
        for (float density : densities) {
            benchmark_sparse_algorithms(N, density);
        }
    }

    return 0;
} </code></pre>
    </div>

    <div class="file-section">
        <h3>strassen.cpp</h3>
        <pre><code class="language-cpp">#include "matrix_mult.h"
#include &lt;hip/hip_runtime.h&gt;

// Define HIP_CHECK macro for error handling
#define HIP_CHECK(call) do {                                                        \
    hipError_t err = call;                                                         \
    if (err != hipSuccess) {                                                       \
        printf("HIP error %s:%d: '%s'\n", __FILE__, __LINE__,                     \
               hipGetErrorString(err));                                            \
        exit(1);                                                                   \
    }                                                                              \
} while(0)

// Kernel for matrix addition
__global__ void matrix_add_kernel(
    const float* A,
    const float* B,
    float* C,
    int N)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; N * N) {
        C[idx] = A[idx] + B[idx];
    }
}

// Kernel for matrix subtraction
__global__ void matrix_sub_kernel(
    const float* A,
    const float* B,
    float* C,
    int N)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; N * N) {
        C[idx] = A[idx] - B[idx];
    }
}

// Helper function to allocate device memory and copy data
void allocate_and_copy(float** d_ptr, const float* h_ptr, size_t size) {
    HIP_CHECK(hipMalloc(d_ptr, size));
    if (h_ptr) {
        HIP_CHECK(hipMemcpy(*d_ptr, h_ptr, size, hipMemcpyHostToDevice));
    }
}

// Helper function to perform matrix addition on device
void matrix_add(const float* A, const float* B, float* C, int N) {
    dim3 block(256);
    dim3 grid((N * N + block.x - 1) / block.x);
    matrix_add_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, N);
}

// Helper function to perform matrix subtraction on device
void matrix_sub(const float* A, const float* B, float* C, int N) {
    dim3 block(256);
    dim3 grid((N * N + block.x - 1) / block.x);
    matrix_sub_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, N);
}

// Recursive Strassen's algorithm implementation
void strassen_recursive(
    float* A, float* B, float* C,
    float* P1, float* P2, float* P3, float* P4, float* P5, float* P6, float* P7,
    float* temp1, float* temp2,
    int N)
{
    if (N &lt;= 64) {  // Base case: use standard matrix multiplication for small matrices
        dim3 block(16, 16);
        dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);
        
        // Simple matrix multiplication kernel
        auto simple_gemm = [&amp;]() {
            for (int i = 0; i &lt; N; i++) {
                for (int j = 0; j &lt; N; j++) {
                    float sum = 0.0f;
                    for (int k = 0; k &lt; N; k++) {
                        sum += A[i * N + k] * B[k * N + j];
                    }
                    C[i * N + j] = sum;
                }
            }
        };
        
        simple_gemm();
        return;
    }

    int new_size = N / 2;
    size_t quarter_size = new_size * new_size * sizeof(float);

    // Allocate temporary matrices for the quadrants
    float *A11, *A12, *A21, *A22;
    float *B11, *B12, *B21, *B22;
    float *C11, *C12, *C21, *C22;
    
    allocate_and_copy(&amp;A11, nullptr, quarter_size);
    allocate_and_copy(&amp;A12, nullptr, quarter_size);
    allocate_and_copy(&amp;A21, nullptr, quarter_size);
    allocate_and_copy(&amp;A22, nullptr, quarter_size);
    allocate_and_copy(&amp;B11, nullptr, quarter_size);
    allocate_and_copy(&amp;B12, nullptr, quarter_size);
    allocate_and_copy(&amp;B21, nullptr, quarter_size);
    allocate_and_copy(&amp;B22, nullptr, quarter_size);
    allocate_and_copy(&amp;C11, nullptr, quarter_size);
    allocate_and_copy(&amp;C12, nullptr, quarter_size);
    allocate_and_copy(&amp;C21, nullptr, quarter_size);
    allocate_and_copy(&amp;C22, nullptr, quarter_size);

    // Split input matrices
    for (int i = 0; i &lt; new_size; i++) {
        for (int j = 0; j &lt; new_size; j++) {
            A11[i * new_size + j] = A[i * N + j];
            A12[i * new_size + j] = A[i * N + j + new_size];
            A21[i * new_size + j] = A[(i + new_size) * N + j];
            A22[i * new_size + j] = A[(i + new_size) * N + j + new_size];
            
            B11[i * new_size + j] = B[i * N + j];
            B12[i * new_size + j] = B[i * N + j + new_size];
            B21[i * new_size + j] = B[(i + new_size) * N + j];
            B22[i * new_size + j] = B[(i + new_size) * N + j + new_size];
        }
    }

    // Compute the seven products
    // P1 = (A11 + A22)(B11 + B22)
    matrix_add(A11, A22, temp1, new_size);
    matrix_add(B11, B22, temp2, new_size);
    strassen_recursive(temp1, temp2, P1, P1, P2, P3, P4, P5, P6, P7, temp1, temp2, new_size);

    // P2 = (A21 + A22)B11
    matrix_add(A21, A22, temp1, new_size);
    strassen_recursive(temp1, B11, P2, P1, P2, P3, P4, P5, P6, P7, temp1, temp2, new_size);

    // P3 = A11(B12 - B22)
    matrix_sub(B12, B22, temp1, new_size);
    strassen_recursive(A11, temp1, P3, P1, P2, P3, P4, P5, P6, P7, temp1, temp2, new_size);

    // P4 = A22(B21 - B11)
    matrix_sub(B21, B11, temp1, new_size);
    strassen_recursive(A22, temp1, P4, P1, P2, P3, P4, P5, P6, P7, temp1, temp2, new_size);

    // P5 = (A11 + A12)B22
    matrix_add(A11, A12, temp1, new_size);
    strassen_recursive(temp1, B22, P5, P1, P2, P3, P4, P5, P6, P7, temp1, temp2, new_size);

    // P6 = (A21 - A11)(B11 + B12)
    matrix_sub(A21, A11, temp1, new_size);
    matrix_add(B11, B12, temp2, new_size);
    strassen_recursive(temp1, temp2, P6, P1, P2, P3, P4, P5, P6, P7, temp1, temp2, new_size);

    // P7 = (A12 - A22)(B21 + B22)
    matrix_sub(A12, A22, temp1, new_size);
    matrix_add(B21, B22, temp2, new_size);
    strassen_recursive(temp1, temp2, P7, P1, P2, P3, P4, P5, P6, P7, temp1, temp2, new_size);

    // Calculate C11, C12, C21, C22
    // C11 = P1 + P4 - P5 + P7
    matrix_add(P1, P4, temp1, new_size);
    matrix_sub(P7, P5, temp2, new_size);
    matrix_add(temp1, temp2, C11, new_size);

    // C12 = P3 + P5
    matrix_add(P3, P5, C12, new_size);

    // C21 = P2 + P4
    matrix_add(P2, P4, C21, new_size);

    // C22 = P1 - P2 + P3 + P6
    matrix_sub(P1, P2, temp1, new_size);
    matrix_add(P3, P6, temp2, new_size);
    matrix_add(temp1, temp2, C22, new_size);

    // Combine results into C
    for (int i = 0; i &lt; new_size; i++) {
        for (int j = 0; j &lt; new_size; j++) {
            C[i * N + j] = C11[i * new_size + j];
            C[i * N + j + new_size] = C12[i * new_size + j];
            C[(i + new_size) * N + j] = C21[i * new_size + j];
            C[(i + new_size) * N + j + new_size] = C22[i * new_size + j];
        }
    }

    // Cleanup
    hipFree(A11); hipFree(A12); hipFree(A21); hipFree(A22);
    hipFree(B11); hipFree(B12); hipFree(B21); hipFree(B22);
    hipFree(C11); hipFree(C12); hipFree(C21); hipFree(C22);
}

PerfResult strassen_multiply(const float* A, const float* B, float* C, int N) {
    PerfResult result = {0.0, 0.0, "Strassen", 0.0};  // Initialize all fields

    // Ensure N is a power of 2
    if ((N &amp; (N - 1)) != 0) {
        // Handle non-power-of-2 sizes by padding
        int new_size = 1;
        while (new_size &lt; N) new_size *= 2;
        
        // Create padded matrices
        std::vector&lt;float&gt; A_padded(new_size * new_size, 0.0f);
        std::vector&lt;float&gt; B_padded(new_size * new_size, 0.0f);
        std::vector&lt;float&gt; C_padded(new_size * new_size, 0.0f);

        // Copy original matrices to padded ones
        for (int i = 0; i &lt; N; i++) {
            for (int j = 0; j &lt; N; j++) {
                A_padded[i * new_size + j] = A[i * N + j];
                B_padded[i * new_size + j] = B[i * N + j];
            }
        }

        // Allocate device memory for padded matrices
        float *d_A, *d_B, *d_C;
        float *d_P1, *d_P2, *d_P3, *d_P4, *d_P5, *d_P6, *d_P7;
        float *d_temp1, *d_temp2;

        size_t size = new_size * new_size * sizeof(float);
        allocate_and_copy(&amp;d_A, A_padded.data(), size);
        allocate_and_copy(&amp;d_B, B_padded.data(), size);
        allocate_and_copy(&amp;d_C, nullptr, size);
        allocate_and_copy(&amp;d_P1, nullptr, size);
        allocate_and_copy(&amp;d_P2, nullptr, size);
        allocate_and_copy(&amp;d_P3, nullptr, size);
        allocate_and_copy(&amp;d_P4, nullptr, size);
        allocate_and_copy(&amp;d_P5, nullptr, size);
        allocate_and_copy(&amp;d_P6, nullptr, size);
        allocate_and_copy(&amp;d_P7, nullptr, size);
        allocate_and_copy(&amp;d_temp1, nullptr, size);
        allocate_and_copy(&amp;d_temp2, nullptr, size);

        // Time the Strassen multiplication
        hipEvent_t start, stop;
        hipEventCreate(&amp;start);
        hipEventCreate(&amp;stop);

        hipEventRecord(start);
        strassen_recursive(d_A, d_B, d_C, d_P1, d_P2, d_P3, d_P4, d_P5, d_P6, d_P7, d_temp1, d_temp2, new_size);
        hipEventRecord(stop);
        hipEventSynchronize(stop);

        float milliseconds = 0;
        hipEventElapsedTime(&amp;milliseconds, start, stop);
        result.time_ms = milliseconds;

        // Copy result back and remove padding
        HIP_CHECK(hipMemcpy(C_padded.data(), d_C, size, hipMemcpyDeviceToHost));
        for (int i = 0; i &lt; N; i++) {
            for (int j = 0; j &lt; N; j++) {
                C[i * N + j] = C_padded[i * new_size + j];
            }
        }

        // Calculate GFLOPS (approximate for Strassen's algorithm)
        double operations = 7.0 * pow(new_size, log2(7)) - 6.0 * new_size * new_size;
        result.gflops = (operations / (milliseconds * 1e-3)) / 1e9;

        // Cleanup
        hipFree(d_A); hipFree(d_B); hipFree(d_C);
        hipFree(d_P1); hipFree(d_P2); hipFree(d_P3); hipFree(d_P4);
        hipFree(d_P5); hipFree(d_P6); hipFree(d_P7);
        hipFree(d_temp1); hipFree(d_temp2);
        hipEventDestroy(start);
        hipEventDestroy(stop);
    } else {
        // For power-of-2 sizes, proceed directly
        float *d_A, *d_B, *d_C;
        float *d_P1, *d_P2, *d_P3, *d_P4, *d_P5, *d_P6, *d_P7;
        float *d_temp1, *d_temp2;

        size_t size = N * N * sizeof(float);
        allocate_and_copy(&amp;d_A, A, size);
        allocate_and_copy(&amp;d_B, B, size);
        allocate_and_copy(&amp;d_C, nullptr, size);
        allocate_and_copy(&amp;d_P1, nullptr, size);
        allocate_and_copy(&amp;d_P2, nullptr, size);
        allocate_and_copy(&amp;d_P3, nullptr, size);
        allocate_and_copy(&amp;d_P4, nullptr, size);
        allocate_and_copy(&amp;d_P5, nullptr, size);
        allocate_and_copy(&amp;d_P6, nullptr, size);
        allocate_and_copy(&amp;d_P7, nullptr, size);
        allocate_and_copy(&amp;d_temp1, nullptr, size);
        allocate_and_copy(&amp;d_temp2, nullptr, size);

        hipEvent_t start, stop;
        hipEventCreate(&amp;start);
        hipEventCreate(&amp;stop);

        hipEventRecord(start);
        strassen_recursive(d_A, d_B, d_C, d_P1, d_P2, d_P3, d_P4, d_P5, d_P6, d_P7, d_temp1, d_temp2, N);
        hipEventRecord(stop);
        hipEventSynchronize(stop);

        float milliseconds = 0;
        hipEventElapsedTime(&amp;milliseconds, start, stop);
        result.time_ms = milliseconds;

        HIP_CHECK(hipMemcpy(C, d_C, size, hipMemcpyDeviceToHost));

        // Calculate GFLOPS (approximate for Strassen's algorithm)
        double operations = 7.0 * pow(N, log2(7)) - 6.0 * N * N;
        result.gflops = (operations / (milliseconds * 1e-3)) / 1e9;

        // Cleanup
        hipFree(d_A); hipFree(d_B); hipFree(d_C);
        hipFree(d_P1); hipFree(d_P2); hipFree(d_P3); hipFree(d_P4);
        hipFree(d_P5); hipFree(d_P6); hipFree(d_P7);
        hipFree(d_temp1); hipFree(d_temp2);
        hipEventDestroy(start);
        hipEventDestroy(stop);
    }

    return result;
} </code></pre>
    </div>

    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-markdown.min.js"></script>
</body>
</html>