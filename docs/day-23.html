<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 23 - GPU Kernels Learning Journey</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 1rem;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .header .subtitle {
            font-size: 1.1rem;
            opacity: 0.9;
        }
        
        .nav {
            background: white;
            padding: 1rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }
        
        .nav a {
            color: #667eea;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: all 0.3s;
        }
        
        .nav a:hover {
            background: #667eea;
            color: white;
        }
        
        .container {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 1rem;
        }
        
        .description {
            background: white;
            padding: 2rem;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            margin-bottom: 2rem;
        }
        
        .description h2 {
            color: #667eea;
            margin-bottom: 1rem;
            border-bottom: 2px solid #667eea;
            padding-bottom: 0.5rem;
        }
        
        .description pre {
            background: #f5f5f5;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        .file-section {
            background: white;
            margin-bottom: 2rem;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        
        .file-section h3 {
            background: #667eea;
            color: white;
            padding: 1rem;
            margin: 0;
        }
        
        .file-section pre {
            margin: 0;
            border-radius: 0;
        }
        
        .file-section code {
            display: block;
            padding: 1.5rem;
            max-height: 600px;
            overflow: auto;
        }
        
        .no-files {
            text-align: center;
            padding: 3rem;
            color: #999;
            font-style: italic;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8rem;
            }
            
            .nav-content {
                flex-direction: column;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Day 23</h1>
        <div class="subtitle">GPU Kernels Learning Journey</div>
    </div>
    
    <nav class="nav">
        <div class="nav-content">
            <a href="index.html">← Back to Index</a>
            <div>
                <a href="day-22.html">← Day 22</a>
                <a href="day-24.html">Day 24 →</a>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="description">
            <h2>Description</h2>
            <div class="desc-content">
### File: `swiglu.cu`
**Summary:**  
Implemented the SwiGLU (Swish-Gated Linear Unit) activation function in CUDA. This kernel computes the activation function in a parallelized manner, enhancing the performance of neural network models that utilize the SwiGLU activation. The implementation focuses on efficient computation and memory access patterns to optimize performance on CUDA-capable GPUs.

**Key Components:**
1. **Kernel Function:**  
   - The `swiglu_kernel` computes the SwiGLU output by first performing matrix multiplications with weight matrices `W1` and `W2`, followed by applying the sigmoid function to the results. This approach allows the kernel to compute the outputs for multiple batches and dimensions concurrently.

2. **Memory Management:**  
   - Memory allocation and deallocation on the GPU are managed using `cudaMalloc` and `cudaFree`, ensuring efficient usage of GPU resources. Input matrices are copied to the device memory using `cudaMemcpy`.

3. **Debugging Information:**  
   - Added debugging print statements within the GPU kernel to help verify correctness for the initial output values. This assists in tracking computations and identifying potential issues during development.

4. **Manual Verification:**  
   - Performed manual computations for the first output element on the CPU to verify the correctness of the CUDA implementation against expectations, ensuring the output matches the sequential computation results.


---
            </div>
        </div>
        
        <h2 style="margin-bottom: 1rem; color: #667eea;">Code Files</h2>

    <div class="file-section">
        <h3>SwiGLU.cu</h3>
        <pre><code class="language-cuda">#include &lt;cuda_runtime.h&gt;
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;random&gt;

// Kernel function for SwiGLU
__global__ void swiglu_kernel(float* out, const float* x, const float* W1, const float* W2, int batch_size, int hidden_dim, int output_dim) {
    int b = blockIdx.x * blockDim.x + threadIdx.x;
    int o = blockIdx.y * blockDim.y + threadIdx.y;

    if (b &lt; batch_size &amp;&amp; o &lt; output_dim) {
        float xW1 = 0.0f;
        float xW2 = 0.0f;
        
        for (int i = 0; i &lt; hidden_dim; i++) {
            xW1 += x[b * hidden_dim + i] * W1[o + i * output_dim];
            xW2 += x[b * hidden_dim + i] * W2[o + i * output_dim];
        }
        
        float sigmoid_val = 1.0f / (1.0f + expf(-xW1));
        float result = xW1 * sigmoid_val * xW2;
        
        if (b == 0 &amp;&amp; o == 0) {  // Print debug info for first element
            printf("GPU Debug: xW1=%f, xW2=%f, sigmoid_val=%f, result=%f\n", 
                   xW1, xW2, sigmoid_val, result);
        }
        
        out[b * output_dim + o] = result;
    }
}

void swiglu_forward(float* out, const float* x, const float* W1, const float* W2, int batch_size, int hidden_dim, int output_dim) {
    // Allocate memory on GPU
    float *d_x, *d_W1, *d_W2, *d_out;
    cudaMalloc((void**)&amp;d_x, batch_size * hidden_dim * sizeof(float));
    cudaMalloc((void**)&amp;d_W1, hidden_dim * output_dim * sizeof(float));
    cudaMalloc((void**)&amp;d_W2, hidden_dim * output_dim * sizeof(float));
    cudaMalloc((void**)&amp;d_out, batch_size * output_dim * sizeof(float));
    
    // Copy data to GPU
    cudaMemcpy(d_x, x, batch_size * hidden_dim * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_W1, W1, hidden_dim * output_dim * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_W2, W2, hidden_dim * output_dim * sizeof(float), cudaMemcpyHostToDevice);
    
    // Define CUDA kernel launch parameters
    dim3 threadsPerBlock(16, 16);
    dim3 blocksPerGrid((batch_size + threadsPerBlock.x - 1) / threadsPerBlock.x,
                       (output_dim + threadsPerBlock.y - 1) / threadsPerBlock.y);
    
    // Launch kernel
    swiglu_kernel&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_out, d_x, d_W1, d_W2, batch_size, hidden_dim, output_dim);
    
    // Check for kernel launch errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        std::cerr &lt;&lt; "Kernel launch failed: " &lt;&lt; cudaGetErrorString(err) &lt;&lt; std::endl;
    }
    
    // Copy result back to CPU
    cudaMemcpy(out, d_out, batch_size * output_dim * sizeof(float), cudaMemcpyDeviceToHost);
    
    // Free GPU memory
    cudaFree(d_x);
    cudaFree(d_W1);
    cudaFree(d_W2);
    cudaFree(d_out);
}

int main() {
    int batch_size = 32;
    int hidden_dim = 128;
    int output_dim = 64;
    
    // Allocate memory
    float *x = new float[batch_size * hidden_dim];
    float *W1 = new float[hidden_dim * output_dim];
    float *W2 = new float[hidden_dim * output_dim];
    float *out = new float[batch_size * output_dim];
    
    // Initialize random number generator
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution&lt;float&gt; dis(0.0f, 1.0f);
    
    // Initialize input data with random values between 0 and 1
    for (int i = 0; i &lt; batch_size * hidden_dim; i++) {
        x[i] = dis(gen);
    }
    for (int i = 0; i &lt; hidden_dim * output_dim; i++) {
        W1[i] = dis(gen);
        W2[i] = dis(gen);
    }
    
    // Manual CPU calculation for first element (for verification)
    float manual_xW1 = 0.0f;
    float manual_xW2 = 0.0f;
    for (int i = 0; i &lt; hidden_dim; i++) {
        manual_xW1 += x[i] * W1[i * output_dim];
        manual_xW2 += x[i] * W2[i * output_dim];
    }
    std::cout &lt;&lt; "CPU Manual calculation for first element:" &lt;&lt; std::endl;
    std::cout &lt;&lt; "xW1: " &lt;&lt; manual_xW1 &lt;&lt; std::endl;
    std::cout &lt;&lt; "xW2: " &lt;&lt; manual_xW2 &lt;&lt; std::endl;
    float manual_sigmoid = 1.0f / (1.0f + exp(-manual_xW1));
    float manual_result = manual_xW1 * manual_sigmoid * manual_xW2;
    std::cout &lt;&lt; "Expected result: " &lt;&lt; manual_result &lt;&lt; std::endl;
    
    // Compute SwiGLU
    swiglu_forward(out, x, W1, W2, batch_size, hidden_dim, output_dim);
    
    // Print some input values
    std::cout &lt;&lt; "\nFirst 10 input values:" &lt;&lt; std::endl;
    for (int i = 0; i &lt; 10; i++) {
        std::cout &lt;&lt; "x[" &lt;&lt; i &lt;&lt; "]: " &lt;&lt; x[i] &lt;&lt; std::endl;
    }
    
    std::cout &lt;&lt; "\nFirst 10 W1 values:" &lt;&lt; std::endl;
    for (int i = 0; i &lt; 10; i++) {
        std::cout &lt;&lt; "W1[" &lt;&lt; i &lt;&lt; "]: " &lt;&lt; W1[i] &lt;&lt; std::endl;
    }
    
    std::cout &lt;&lt; "\nFirst 10 W2 values:" &lt;&lt; std::endl;
    for (int i = 0; i &lt; 10; i++) {
        std::cout &lt;&lt; "W2[" &lt;&lt; i &lt;&lt; "]: " &lt;&lt; W2[i] &lt;&lt; std::endl;
    }
    
    // Print output values
    std::cout &lt;&lt; "\nFirst 10 output values:" &lt;&lt; std::endl;
    for (int i = 0; i &lt; 10; i++) {
        std::cout &lt;&lt; "out[" &lt;&lt; i &lt;&lt; "]: " &lt;&lt; out[i] &lt;&lt; std::endl;
    }
    
    // Free memory
    delete[] x;
    delete[] W1;
    delete[] W2;
    delete[] out;
    
    return 0;
}
</code></pre>
    </div>

    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-markdown.min.js"></script>
</body>
</html>