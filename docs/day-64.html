<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 64 - GPU Kernels Learning Journey</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 1rem;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .header .subtitle {
            font-size: 1.1rem;
            opacity: 0.9;
        }
        
        .nav {
            background: white;
            padding: 1rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }
        
        .nav a {
            color: #667eea;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: all 0.3s;
        }
        
        .nav a:hover {
            background: #667eea;
            color: white;
        }
        
        .container {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 1rem;
        }
        
        .description {
            background: white;
            padding: 2rem;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            margin-bottom: 2rem;
        }
        
        .description h2 {
            color: #667eea;
            margin-bottom: 1rem;
            border-bottom: 2px solid #667eea;
            padding-bottom: 0.5rem;
        }
        
        .description pre {
            background: #f5f5f5;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        .file-section {
            background: white;
            margin-bottom: 2rem;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        
        .file-section h3 {
            background: #667eea;
            color: white;
            padding: 1rem;
            margin: 0;
        }
        
        .file-section pre {
            margin: 0;
            border-radius: 0;
        }
        
        .file-section code {
            display: block;
            padding: 1.5rem;
            max-height: 600px;
            overflow: auto;
        }
        
        .no-files {
            text-align: center;
            padding: 3rem;
            color: #999;
            font-style: italic;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8rem;
            }
            
            .nav-content {
                flex-direction: column;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Day 64</h1>
        <div class="subtitle">GPU Kernels Learning Journey</div>
    </div>
    
    <nav class="nav">
        <div class="nav-content">
            <a href="index.html">← Back to Index</a>
            <div>
                <a href="day-63.html">← Day 63</a>
                <a href="day-65.html">Day 65 →</a>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="description">
            <h2>Description</h2>
            <div class="desc-content">
### Files: `kl_divergence.cu`

Today, I implemented the KL divergence loss (forward and backward passes) in CUDA, inspired by the Liger Triton kernel implementation. This CUDA version is designed to mirror the functionality of the Triton kernel but leverages native CUDA C++ for potentially improved performance and greater control over kernel behavior.
            </div>
        </div>
        
        <h2 style="margin-bottom: 1rem; color: #667eea;">Code Files</h2>

    <div class="file-section">
        <h3>kl_divergence.cu</h3>
        <pre><code class="language-cuda">
#include &lt;cuda_runtime.h&gt;
#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

// Reduction modes.
enum Reduction { NONE = 0, SUM = 1, MEAN = 2, BATCHMEAN = 3 };

// -------------------------------------------------------------------
// Forward kernels
// -------------------------------------------------------------------

__global__ void kldiv_forward_kernel_none(const float* __restrict__ y_pred,
                                            const float* __restrict__ y_true,
                                            float* __restrict__ loss,
                                            int V,
                                            float eps,
                                            bool log_target) {
    int b = blockIdx.x;  
    int i = threadIdx.x;
    int offset = b * V + i;
    if (i &lt; V) {
        float pred = y_pred[offset];
        float target = y_true[offset];
        float val = 0.0f;
        if (!log_target) {
            
            val = target * (logf(fmaxf(target, eps)) - pred);
        } else {
            val = expf(target) * (target - pred);
        }
        loss[offset] = val;
    }
}


__global__ void kldiv_forward_kernel_reduce(const float* __restrict__ y_pred,
                                              const float* __restrict__ y_true,
                                              float* __restrict__ loss,
                                              int V,
                                              float eps,
                                              bool log_target) {
    int b = blockIdx.x; 
    extern __shared__ float sdata[]; 
    float sum = 0.0f;

   
    for (int i = threadIdx.x; i &lt; V; i += blockDim.x) {
        int offset = b * V + i;
        float pred = y_pred[offset];
        float target = y_true[offset];
        float val = 0.0f;
        if (!log_target) {
            val = target * (logf(fmaxf(target, eps)) - pred);
        } else {
            val = expf(target) * (target - pred);
        }
        sum += val;
    }
    sdata[threadIdx.x] = sum;
    __syncthreads();

   
    for (int s = blockDim.x / 2; s &gt; 0; s &gt;&gt;= 1) {
        if (threadIdx.x &lt; s) {
            sdata[threadIdx.x] += sdata[threadIdx.x + s];
        }
        __syncthreads();
    }
    
    if (threadIdx.x == 0) {
        loss[b] = sdata[0];
    }
}

// -------------------------------------------------------------------
// Backward kernels
// -------------------------------------------------------------------

__global__ void kldiv_backward_kernel(const float* __restrict__ y_true,
                                      float* __restrict__ grad,
                                      int V,
                                      bool log_target) {
    int b = blockIdx.x;  // batch index
    for (int i = threadIdx.x; i &lt; V; i += blockDim.x) {
        int offset = b * V + i;
        float target = y_true[offset];
        float res = (!log_target) ? -target : -expf(target);
        grad[offset] = res;
    }
}


__global__ void scale_kernel(float* data, int N, float factor) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &lt; N) {
        data[i] *= factor;
    }
}

// -------------------------------------------------------------------
// Host functions for forward and backward passes
// -------------------------------------------------------------------

// Forward pass host function.
void kldiv_forward(const float* y_pred, const float* y_true, float* loss,
                   int B, int V, float eps, bool log_target, Reduction reduction) {
    if (reduction == NONE) {
       
        int threads = V;
        dim3 grid(B);
        kldiv_forward_kernel_none&lt;&lt;&lt;grid, threads&gt;&gt;&gt;(y_pred, y_true, loss, V, eps, log_target);
    } else {
       
        int threads = 256;
        dim3 grid(B);
        size_t shared_mem_size = threads * sizeof(float);
        kldiv_forward_kernel_reduce&lt;&lt;&lt;grid, threads, shared_mem_size&gt;&gt;&gt;(y_pred, y_true, loss, V, eps, log_target);
    }
    cudaDeviceSynchronize();
}

// Backward pass host function.
void kldiv_backward(const float* y_true, float* grad,
                    int B, int V, bool log_target, float grad_output = 1.0f) {
    int threads = 256;
    dim3 grid(B);
    kldiv_backward_kernel&lt;&lt;&lt;grid, threads&gt;&gt;&gt;(y_true, grad, V, log_target);
    cudaDeviceSynchronize();

    // If grad_output is not 1, we scale the gradients!
    if (grad_output != 1.0f) {
        int total = B * V;
        int blockSize = 256;
        int numBlocks = (total + blockSize - 1) / blockSize;
        scale_kernel&lt;&lt;&lt;numBlocks, blockSize&gt;&gt;&gt;(grad, total, grad_output);
        cudaDeviceSynchronize();
    }
}


int main() {
    
    const int B = 2;
    const int V = 1024;
    size_t dataSize = B * V * sizeof(float);
    size_t lossSize = (B * ((BATCHMEAN == NONE) ? V : 1)) * sizeof(float);

   
    float* h_y_pred = new float[B * V];
    float* h_y_true = new float[B * V];
    float* h_loss   = new float[B]; 

    // Initialize
    for (int i = 0; i &lt; B * V; i++) {
        h_y_pred[i] = 0.5f; 
        h_y_true[i] = 0.3f; 
    }

  
    float *d_y_pred, *d_y_true, *d_loss, *d_grad;
    cudaMalloc(&amp;d_y_pred, dataSize);
    cudaMalloc(&amp;d_y_true, dataSize);
 
    cudaMalloc(&amp;d_loss, B * sizeof(float));
    cudaMalloc(&amp;d_grad, dataSize);

  
    cudaMemcpy(d_y_pred, h_y_pred, dataSize, cudaMemcpyHostToDevice);
    cudaMemcpy(d_y_true, h_y_true, dataSize, cudaMemcpyHostToDevice);

    // forward pass.
    Reduction reduction = BATCHMEAN;
    kldiv_forward(d_y_pred, d_y_true, d_loss, B, V, 1e-10f, false, reduction);

   
    cudaMemcpy(h_loss, d_loss, B * sizeof(float), cudaMemcpyDeviceToHost);
    printf("Forward loss (per batch):\n");
    for (int b = 0; b &lt; B; b++) {
        printf("Batch %d: %f\n", b, h_loss[b]);
    }

    // backward pass.
    // This computes the gradient based on y_true.
    kldiv_backward(d_y_true, d_grad, B, V, false, 1.0f);

    
    float* h_grad = new float[B * V];
    cudaMemcpy(h_grad, d_grad, dataSize, cudaMemcpyDeviceToHost);
    printf("\nFirst 10 gradient values:\n");
    for (int i = 0; i &lt; 10; i++) {
        printf("%f ", h_grad[i]);
    }
    printf("\n");


    cudaFree(d_y_pred);
    cudaFree(d_y_true);
    cudaFree(d_loss);
    cudaFree(d_grad);


    delete[] h_y_pred;
    delete[] h_y_true;
    delete[] h_loss;
    delete[] h_grad;

    return 0;
}
</code></pre>
    </div>

    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-markdown.min.js"></script>
</body>
</html>