<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 9 - GPU Kernels Learning Journey</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 1rem;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .header .subtitle {
            font-size: 1.1rem;
            opacity: 0.9;
        }
        
        .nav {
            background: white;
            padding: 1rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }
        
        .nav a {
            color: #667eea;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: all 0.3s;
        }
        
        .nav a:hover {
            background: #667eea;
            color: white;
        }
        
        .container {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 1rem;
        }
        
        .description {
            background: white;
            padding: 2rem;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            margin-bottom: 2rem;
        }
        
        .description h2 {
            color: #667eea;
            margin-bottom: 1rem;
            border-bottom: 2px solid #667eea;
            padding-bottom: 0.5rem;
        }
        
        .description pre {
            background: #f5f5f5;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        .file-section {
            background: white;
            margin-bottom: 2rem;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        
        .file-section h3 {
            background: #667eea;
            color: white;
            padding: 1rem;
            margin: 0;
        }
        
        .file-section pre {
            margin: 0;
            border-radius: 0;
        }
        
        .file-section code {
            display: block;
            padding: 1.5rem;
            max-height: 600px;
            overflow: auto;
        }
        
        .no-files {
            text-align: center;
            padding: 3rem;
            color: #999;
            font-style: italic;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8rem;
            }
            
            .nav-content {
                flex-direction: column;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Day 9</h1>
        <div class="subtitle">GPU Kernels Learning Journey</div>
    </div>
    
    <nav class="nav">
        <div class="nav-content">
            <a href="index.html">← Back to Index</a>
            <div>
                <a href="day-8.html">← Day 8</a>
                <a href="day-10.html">Day 10 →</a>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="description">
            <h2>Description</h2>
            <div class="desc-content">
<p>No description available for this day.</p>
            </div>
        </div>
        
        <h2 style="margin-bottom: 1rem; color: #667eea;">Code Files</h2>

    <div class="file-section">
        <h3>flash_attention_forward.cu</h3>
        <pre><code class="language-cuda">#include &lt;iostream&gt;
#include &lt;cuda_runtime.h&gt;
#include &lt;math_constants.h&gt;
#include &lt;cmath&gt;

#define SRAM_SIZE 1024                    // M: SRAM size
#define sequence_length 2              // N: sequence length
#define embed_dimension 2              // d: embedding dimension

// Define constant sizes to be used for block sizes
constexpr int Block_column_size = SRAM_SIZE / (4 * embed_dimension); // Bc
constexpr int Block_row_size = std::min(SRAM_SIZE / (4 * embed_dimension), embed_dimension); // Br

// Ensure we don't have a division by zero situation
static_assert(Block_column_size &gt; 0, "Block_column_size must be greater than 0");
static_assert(Block_row_size &gt; 0, "Block_row_size must be greater than 0");

constexpr int Total_row_blocks = (sequence_length + Block_row_size - 1) / Block_row_size; // Tr
constexpr int Total_column_blocks = (sequence_length + Block_column_size - 1) / Block_column_size; // Tc

__global__ void flashAttentionForward(
    const float *Query,                 // Q
    const float *Key,                   // K
    const float *Value,                 // V
    float *Output,                      // O
    float *max_values,                  // m
    float *sum_values,                  // l
    const float attention_scale)        // 1/sqrt(d)
{
    int thread_idx = threadIdx.x;

    // Thread-local storage for attention scores and weights
    float attention_scores[Block_row_size * Block_column_size];
    float attention_weights[Block_row_size * Block_column_size];

    // Shared memory blocks
    float Query_block[Block_row_size * embed_dimension];
    float Key_block[Block_column_size * embed_dimension];
    float Value_block[Block_column_size * embed_dimension];

    for (int col_block = 0; col_block &lt; Total_column_blocks; ++col_block)
    {
        // Load Key and Value blocks from global memory
        if (thread_idx &lt; Block_column_size) {
            for (int d = 0; d &lt; embed_dimension; ++d) {
                Key_block[thread_idx * embed_dimension + d] = 
                    Key[col_block * Block_column_size * embed_dimension + thread_idx * embed_dimension + d];
                Value_block[thread_idx * embed_dimension + d] = 
                    Value[col_block * Block_column_size * embed_dimension + thread_idx * embed_dimension + d];
            }
        }
        __syncthreads();

        for (int row_block = 0; row_block &lt; Total_row_blocks; ++row_block)
        {
            if (thread_idx &lt; Block_row_size) {
                // Load Query block
                for (int d = 0; d &lt; embed_dimension; ++d) {
                    Query_block[thread_idx * embed_dimension + d] = 
                        Query[row_block * Block_row_size * embed_dimension + thread_idx * embed_dimension + d];
                }
            }
            __syncthreads();

            // Compute attention scores for this row
            if (thread_idx &lt; Block_row_size) {
                float row_max = -1e20;  // Use a large negative float
                for (int k = 0; k &lt; Block_column_size; ++k) {
                    float score = 0.0f;
                    for (int d = 0; d &lt; embed_dimension; ++d) {
                        score += Query_block[thread_idx * embed_dimension + d] * 
                                Key_block[k * embed_dimension + d];
                    }
                    score *= attention_scale;
                    attention_scores[thread_idx * Block_column_size + k] = score;
                    row_max = fmaxf(row_max, score);
                }

                // Compute attention weights with softmax
                float row_sum = 0.0f;
                for (int k = 0; k &lt; Block_column_size; ++k) {
                    float weight = expf(attention_scores[thread_idx * Block_column_size + k] - row_max);
                    attention_weights[thread_idx * Block_column_size + k] = weight;
                    row_sum += weight;
                }

                // Update output
                for (int d = 0; d &lt; embed_dimension; ++d) {
                    float weighted_sum = 0.0f;
                    for (int k = 0; k &lt; Block_column_size; ++k) {
                        weighted_sum += attention_weights[thread_idx * Block_column_size + k] * 
                                      Value_block[k * embed_dimension + d];
                    }
                    Output[row_block * Block_row_size * embed_dimension + thread_idx * embed_dimension + d] = 
                        (row_sum &gt; 0) ? (weighted_sum / row_sum) : 0; // Avoid division by zero
                }
            }
            __syncthreads();
        }
    }
}

int main()
{
    // Host memory allocation
    float (*Query)[embed_dimension] = new float[sequence_length][embed_dimension];
    float (*Key)[embed_dimension] = new float[sequence_length][embed_dimension];
    float (*Value)[embed_dimension] = new float[sequence_length][embed_dimension];
    float (*Output)[embed_dimension] = new float[sequence_length][embed_dimension];
    float *sum_values = new float[sequence_length]();  // Initialize to zeros
    float *max_values = new float[sequence_length];

    // Initialize max_values to a very small negative number
    for (int i = 0; i &lt; sequence_length; i++) {
        max_values[i] = -1e20;  // Large negative float
    }

    // Initialization (random values between -1 and 1): 
    for (int i = 0; i &lt; sequence_length; i++) {
        for (int j = 0; j &lt; embed_dimension; j++) {
            Query[i][j] = 2.0f * rand() / RAND_MAX - 1.0f;
            Key[i][j] = 2.0f * rand() / RAND_MAX - 1.0f;
            Value[i][j] = 2.0f * rand() / RAND_MAX - 1.0f;
            Output[i][j] = 0.0f;  // Initialize output to zeros
        }
    }

    // Device memory pointers
    float *device_Query, *device_Key, *device_Value, *device_Output;
    float *device_max_values, *device_sum_values;

    // Allocate device memory
    cudaMalloc(&amp;device_Query, sequence_length * embed_dimension * sizeof(float));
    cudaMalloc(&amp;device_Key, sequence_length * embed_dimension * sizeof(float));
    cudaMalloc(&amp;device_Value, sequence_length * embed_dimension * sizeof(float));
    cudaMalloc(&amp;device_Output, sequence_length * embed_dimension * sizeof(float));
    cudaMalloc(&amp;device_sum_values, sequence_length * sizeof(float));
    cudaMalloc(&amp;device_max_values, sequence_length * sizeof(float));

    // Copy data from host to device
    cudaMemcpy(device_Query, Query, 
               sequence_length * embed_dimension * sizeof(float), 
               cudaMemcpyHostToDevice);
    cudaMemcpy(device_Key, Key, 
               sequence_length * embed_dimension * sizeof(float), 
               cudaMemcpyHostToDevice);
    cudaMemcpy(device_Value, Value, 
               sequence_length * embed_dimension * sizeof(float), 
               cudaMemcpyHostToDevice);
    cudaMemcpy(device_Output, Output, 
               sequence_length * embed_dimension * sizeof(float), 
               cudaMemcpyHostToDevice);
    cudaMemcpy(device_sum_values, sum_values, 
               sequence_length * sizeof(float), 
               cudaMemcpyHostToDevice);
    cudaMemcpy(device_max_values, max_values, 
               sequence_length * sizeof(float), 
               cudaMemcpyHostToDevice);

    // Calculate attention scaling factor
    float attention_scale = 1.0f / sqrt(embed_dimension);

    // Launch configuration
    dim3 block_dim(Block_row_size);  // One thread per row in Query block
    dim3 grid_dim(1);                // Single block for simplicity

    // Launch kernel
    flashAttentionForward&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(
        device_Query,
        device_Key,
        device_Value,
        device_Output,
        device_max_values,
        device_sum_values,
        attention_scale
    );

    // Check for kernel launch errors
    cudaError_t cudaStatus = cudaGetLastError();
    if (cudaStatus != cudaSuccess) {
        fprintf(stderr, "Kernel launch failed: %s\n", cudaGetErrorString(cudaStatus));
        goto Error;
    }

    // Copy results back to host
    cudaMemcpy(Output, device_Output, 
               sequence_length * embed_dimension * sizeof(float), 
               cudaMemcpyDeviceToHost);
    cudaMemcpy(max_values, device_max_values, 
               sequence_length * sizeof(float), 
               cudaMemcpyDeviceToHost);
    cudaMemcpy(sum_values, device_sum_values, 
               sequence_length * sizeof(float), 
               cudaMemcpyDeviceToHost);



// Print Query
std::cout &lt;&lt; "Query:" &lt;&lt; std::endl;
for (int i = 0; i &lt; sequence_length; i++) {
    for (int j = 0; j &lt; embed_dimension; j++) {
        std::cout &lt;&lt; Query[i][j] &lt;&lt; " ";
    }
    std::cout &lt;&lt; std::endl;
}

// Print Key
std::cout &lt;&lt; "Key:" &lt;&lt; std::endl;
for (int i = 0; i &lt; sequence_length; i++) {
    for (int j = 0; j &lt; embed_dimension; j++) {
        std::cout &lt;&lt; Key[i][j] &lt;&lt; " ";
    }
    std::cout &lt;&lt; std::endl;
}

// Print Value
std::cout &lt;&lt; "Value:" &lt;&lt; std::endl;
for (int i = 0; i &lt; sequence_length; i++) {
    for (int j = 0; j &lt; embed_dimension; j++) {
        std::cout &lt;&lt; Value[i][j] &lt;&lt; " ";
    }
    std::cout &lt;&lt; std::endl;
}

// Print Output
std::cout &lt;&lt; "Output:" &lt;&lt; std::endl;
for (int i = 0; i &lt; sequence_length; i++) {
    for (int j = 0; j &lt; embed_dimension; j++) {
        std::cout &lt;&lt; Output[i][j] &lt;&lt; " ";
    }
    std::cout &lt;&lt; std::endl;
}

Error:
    // Cleanup
    // Device memory
    cudaFree(device_Query);
    cudaFree(device_Key);
    cudaFree(device_Value);
    cudaFree(device_Output);
    cudaFree(device_max_values);
    cudaFree(device_sum_values);

    // Host memory
    delete[] Query;
    delete[] Key;
    delete[] Value;
    delete[] Output;
    delete[] sum_values;
    delete[] max_values;

    return cudaStatus == cudaSuccess ? 0 : 1;
}
</code></pre>
    </div>

    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-markdown.min.js"></script>
</body>
</html>