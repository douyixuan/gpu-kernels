<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 49 - GPU Kernels Learning Journey</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 1rem;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .header .subtitle {
            font-size: 1.1rem;
            opacity: 0.9;
        }
        
        .nav {
            background: white;
            padding: 1rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }
        
        .nav a {
            color: #667eea;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: all 0.3s;
        }
        
        .nav a:hover {
            background: #667eea;
            color: white;
        }
        
        .container {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 1rem;
        }
        
        .description {
            background: white;
            padding: 2rem;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            margin-bottom: 2rem;
        }
        
        .description h2 {
            color: #667eea;
            margin-bottom: 1rem;
            border-bottom: 2px solid #667eea;
            padding-bottom: 0.5rem;
        }
        
        .description pre {
            background: #f5f5f5;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        .file-section {
            background: white;
            margin-bottom: 2rem;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        
        .file-section h3 {
            background: #667eea;
            color: white;
            padding: 1rem;
            margin: 0;
        }
        
        .file-section pre {
            margin: 0;
            border-radius: 0;
        }
        
        .file-section code {
            display: block;
            padding: 1.5rem;
            max-height: 600px;
            overflow: auto;
        }
        
        .no-files {
            text-align: center;
            padding: 3rem;
            color: #999;
            font-style: italic;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8rem;
            }
            
            .nav-content {
                flex-direction: column;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Day 49</h1>
        <div class="subtitle">GPU Kernels Learning Journey</div>
    </div>
    
    <nav class="nav">
        <div class="nav-content">
            <a href="index.html">← Back to Index</a>
            <div>
                <a href="day-48.html">← Day 48</a>
                <a href="day-50.html">Day 50 →</a>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="description">
            <h2>Description</h2>
            <div class="desc-content">
### Files: `cgm.cu`

I implemented a CUDA-based Conjugate Gradient Method (CGM) solver to efficiently solve linear systems Ax=b by iteratively updating search directions without computing Hessians, leveraging GPU parallelism. The implementation includes custom CUDA kernels for matrix–vector multiplication, vector arithmetic (addition, subtraction, scaling), and a shared memory reduction for dot products, ensuring high-performance execution. This approach optimizes memory access and computation, making it suitable for large-scale problems in machine learning and scientific computing while demonstrating how iterative solvers can benefit from GPU acceleration.
            </div>
        </div>
        
        <h2 style="margin-bottom: 1rem; color: #667eea;">Code Files</h2>

    <div class="file-section">
        <h3>CGM.cu</h3>
        <pre><code class="language-cuda">#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;cuda_runtime.h&gt;

// Matrix size (for demonstration purposes)
#define N 4

// CUDA error-checking macro
#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if(err != cudaSuccess) { \
            std::cerr &lt;&lt; "CUDA error in " &lt;&lt; __FILE__ &lt;&lt; "@" &lt;&lt; __LINE__ &lt;&lt; ": " \
                      &lt;&lt; cudaGetErrorString(err) &lt;&lt; std::endl; \
            exit(EXIT_FAILURE); \
        } \
    } while(0)

// Kernel: Matrix-Vector multiplication: y = A*x
__global__ void matVecMul(const float *A, const float *x, float *y, int n) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row &lt; n) {
        float sum = 0.0f;
        for (int j = 0; j &lt; n; j++) {
            sum += A[row * n + j] * x[j];
        }
        y[row] = sum;
    }
}

// Kernel: Vector addition: y = y + alpha * x
__global__ void vecAdd(float *y, const float *x, float alpha, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &lt; n) {
        y[i] += alpha * x[i];
    }
}

// Kernel: Vector subtraction: y = y - alpha * x
__global__ void vecSub(float *y, const float *x, float alpha, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &lt; n) {
        y[i] -= alpha * x[i];
    }
}

// Kernel: Scale vector: x = beta * x
__global__ void vecScale(float *x, float beta, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &lt; n) {
        x[i] *= beta;
    }
}

// Kernel: Dot product using shared memory reduction
__global__ void dotProduct(const float *a, const float *b, float *result, int n) {
    __shared__ float cache[256];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    float temp = 0.0f;
    while (i &lt; n) {
        temp += a[i] * b[i];
        i += blockDim.x * gridDim.x;
    }
    cache[tid] = temp;
    __syncthreads();

    // Reduction in shared memory
    int blockSize = blockDim.x;
    while (blockSize &gt; 1) {
        int halfPoint = blockSize / 2;
        if (tid &lt; halfPoint) {
            cache[tid] += cache[tid + halfPoint];
        }
        __syncthreads();
        blockSize = halfPoint;
    }
    if (tid == 0) {
        atomicAdd(result, cache[0]);
    }
}

int main() {
    const int n = N;
    const int matrixSize = n * n * sizeof(float);
    const int vectorSize = n * sizeof(float);

    // Host data: Define a symmetric positive-definite matrix A and vector b.
    float h_A[N * N] = {
         4, 1, 0, 0,
         1, 3, 1, 0,
         0, 1, 2, 1,
         0, 0, 1, 1
    };
    float h_b[N] = {15, 10, 10, 10};
    float h_x[N] = {0}; // Initial guess x = 0

    // Device allocations
    float *d_A, *d_x, *d_b, *d_r, *d_p, *d_Ap;
    CUDA_CHECK(cudaMalloc(&amp;d_A, matrixSize));
    CUDA_CHECK(cudaMalloc(&amp;d_x, vectorSize));
    CUDA_CHECK(cudaMalloc(&amp;d_b, vectorSize));
    CUDA_CHECK(cudaMalloc(&amp;d_r, vectorSize));
    CUDA_CHECK(cudaMalloc(&amp;d_p, vectorSize));
    CUDA_CHECK(cudaMalloc(&amp;d_Ap, vectorSize));

    // Copy data to device
    CUDA_CHECK(cudaMemcpy(d_A, h_A, matrixSize, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_b, h_b, vectorSize, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_x, h_x, vectorSize, cudaMemcpyHostToDevice));

    // Initialize: r = b - A*x (x=0 =&gt; r=b), p = r
    CUDA_CHECK(cudaMemcpy(d_r, d_b, vectorSize, cudaMemcpyDeviceToDevice));
    CUDA_CHECK(cudaMemcpy(d_p, d_r, vectorSize, cudaMemcpyDeviceToDevice));

    // Allocate device memory for dot product result
    float *d_dot;
    CUDA_CHECK(cudaMalloc(&amp;d_dot, sizeof(float)));

    float rdotr = 0.0f, new_rdotr = 0.0f;

    // Compute initial dot product: rdotr = r^T r
    CUDA_CHECK(cudaMemset(d_dot, 0, sizeof(float)));
    dotProduct&lt;&lt;&lt;1, 256&gt;&gt;&gt;(d_r, d_r, d_dot, n);
    CUDA_CHECK(cudaMemcpy(&amp;rdotr, d_dot, sizeof(float), cudaMemcpyDeviceToHost));

    int max_iter = 1000;
    float tol = 1e-6f;
    int k = 0;

    while (sqrt(rdotr) &gt; tol &amp;&amp; k &lt; max_iter) {
        // Ap = A * p
        matVecMul&lt;&lt;&lt;(n + 255) / 256, 256&gt;&gt;&gt;(d_A, d_p, d_Ap, n);

        // Compute p^T * A * p
        CUDA_CHECK(cudaMemset(d_dot, 0, sizeof(float)));
        dotProduct&lt;&lt;&lt;1, 256&gt;&gt;&gt;(d_p, d_Ap, d_dot, n);
        float pAp = 0.0f;
        CUDA_CHECK(cudaMemcpy(&amp;pAp, d_dot, sizeof(float), cudaMemcpyDeviceToHost));

        float alpha = rdotr / pAp;

        // x = x + alpha * p
        vecAdd&lt;&lt;&lt;(n + 255) / 256, 256&gt;&gt;&gt;(d_x, d_p, alpha, n);

        // r = r - alpha * Ap
        vecSub&lt;&lt;&lt;(n + 255) / 256, 256&gt;&gt;&gt;(d_r, d_Ap, alpha, n);

        // Compute new dot product: new_rdotr = r^T * r
        CUDA_CHECK(cudaMemset(d_dot, 0, sizeof(float)));
        dotProduct&lt;&lt;&lt;1, 256&gt;&gt;&gt;(d_r, d_r, d_dot, n);
        CUDA_CHECK(cudaMemcpy(&amp;new_rdotr, d_dot, sizeof(float), cudaMemcpyDeviceToHost));

        if (sqrt(new_rdotr) &lt; tol) {
            break;
        }

        float beta = new_rdotr / rdotr;

        // p = r + beta * p
        // First scale p: p = beta * p
        vecScale&lt;&lt;&lt;(n + 255) / 256, 256&gt;&gt;&gt;(d_p, beta, n);
        // Then add r: p = p + r
        vecAdd&lt;&lt;&lt;(n + 255) / 256, 256&gt;&gt;&gt;(d_p, d_r, 1.0f, n);

        rdotr = new_rdotr;
        k++;
    }

    // Copy the solution back to host
    CUDA_CHECK(cudaMemcpy(h_x, d_x, vectorSize, cudaMemcpyDeviceToHost));
    std::cout &lt;&lt; "Conjugate Gradient converged in " &lt;&lt; k &lt;&lt; " iterations." &lt;&lt; std::endl;
    std::cout &lt;&lt; "Solution x:" &lt;&lt; std::endl;
    for (int i = 0; i &lt; n; i++) {
        std::cout &lt;&lt; h_x[i] &lt;&lt; std::endl;
    }

    // Cleanup
    CUDA_CHECK(cudaFree(d_A));
    CUDA_CHECK(cudaFree(d_x));
    CUDA_CHECK(cudaFree(d_b));
    CUDA_CHECK(cudaFree(d_r));
    CUDA_CHECK(cudaFree(d_p));
    CUDA_CHECK(cudaFree(d_Ap));
    CUDA_CHECK(cudaFree(d_dot));

    return 0;
}
</code></pre>
    </div>

    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-markdown.min.js"></script>
</body>
</html>