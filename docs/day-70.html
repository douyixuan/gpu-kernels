<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 70 - GPU Kernels Learning Journey</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 1rem;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .header .subtitle {
            font-size: 1.1rem;
            opacity: 0.9;
        }
        
        .nav {
            background: white;
            padding: 1rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }
        
        .nav a {
            color: #667eea;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: all 0.3s;
        }
        
        .nav a:hover {
            background: #667eea;
            color: white;
        }
        
        .container {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 1rem;
        }
        
        .description {
            background: white;
            padding: 2rem;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            margin-bottom: 2rem;
        }
        
        .description h2 {
            color: #667eea;
            margin-bottom: 1rem;
            border-bottom: 2px solid #667eea;
            padding-bottom: 0.5rem;
        }
        
        .description pre {
            background: #f5f5f5;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        .file-section {
            background: white;
            margin-bottom: 2rem;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        
        .file-section h3 {
            background: #667eea;
            color: white;
            padding: 1rem;
            margin: 0;
        }
        
        .file-section pre {
            margin: 0;
            border-radius: 0;
        }
        
        .file-section code {
            display: block;
            padding: 1.5rem;
            max-height: 600px;
            overflow: auto;
        }
        
        .no-files {
            text-align: center;
            padding: 3rem;
            color: #999;
            font-style: italic;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8rem;
            }
            
            .nav-content {
                flex-direction: column;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Day 70</h1>
        <div class="subtitle">GPU Kernels Learning Journey</div>
    </div>
    
    <nav class="nav">
        <div class="nav-content">
            <a href="index.html">← Back to Index</a>
            <div>
                <a href="day-69.html">← Day 69</a>
                <a href="day-71.html">Day 71 →</a>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="description">
            <h2>Description</h2>
            <div class="desc-content">
<h3>Files: <code>tv_distance.cu</code></h3>
<p>Today I implemented the Total Variation Distance (TVD) Loss in CUDA C++ along with its backward pass for gradient computation. The kernel computes the TVD as [TVD = 0.5 x |p - q|] and calculates corresponding gradients by comparing ( p ) and ( q ). The implementation supports optional label-based ignoring (using an <code>ignore_index</code>) and offers multiple reduction options ("none", "sum", "mean", and "batchmean"). In addition to the GPU kernel, I provided a CPU version for benchmarking. The code benchmarks GPU execution (using CUDA events) against a CPU implementation (using C++ chrono) to highlight performance gains and validate correctness.</p>
            </div>
        </div>
        
        <h2 style="margin-bottom: 1rem; color: #667eea;">Code Files</h2>

    <div class="file-section">
        <h3>tv_distance.cu</h3>
        <pre><code class="language-cpp">#include &lt;cuda_runtime.h&gt;
#include &lt;cstdio&gt;
#include &lt;cstdlib&gt;
#include &lt;cmath&gt;
#include &lt;chrono&gt;
#include &lt;iostream&gt;

enum ReductionMode {
    NONE = 0,
    SUM = 1,
    MEAN = 2,
    BATCHMEAN = 3
};

#define CUDA_CHECK(call)                                                      \
    do {                                                                      \
        cudaError_t err = call;                                               \
        if (err != cudaSuccess) {                                             \
            fprintf(stderr, "CUDA error in %s (%s:%d): %s\n",                 \
                    __func__, __FILE__, __LINE__, cudaGetErrorString(err));   \
            exit(err);                                                        \
        }                                                                     \
    } while (0)

__global__ void tv_distance_kernel(
    const float* __restrict__ p,
    int p_stride,
    const float* __restrict__ q,
    int q_stride,
    float* __restrict__ loss,
    int loss_stride,
    float* __restrict__ grads,
    int grads_stride,
    const int* __restrict__ labels,
    int ignore_index,
    int n_cols,
    int reduction,
    bool hasLabel)
{
    int row = blockIdx.x;
    const float* p_row = p + row * p_stride;
    const float* q_row = q + row * q_stride;
    float* loss_row = loss + row * loss_stride;
    float* grads_row = grads + row * grads_stride;

    if (hasLabel) {
        int label = labels[row];
        if (label == ignore_index) {
            for (int col = threadIdx.x; col &lt; n_cols; col += blockDim.x) {
                grads_row[col] = 0.0f;
                if (reduction == NONE) {
                    loss_row[col] = 0.0f;
                }
            }
            return;
        }
    }

    float thread_sum = 0.0f;
    for (int col = threadIdx.x; col &lt; n_cols; col += blockDim.x) {
        float p_val = p_row[col];
        float q_val = q_row[col];
        float tv_loss = 0.5f * fabsf(p_val - q_val);
        float grad = (p_val &gt; q_val) ? 0.5f : -0.5f;
        grads_row[col] = grad;

        if (reduction == NONE) {
            loss_row[col] = tv_loss;
        } else {
            thread_sum += tv_loss;
        }
    }

    if (reduction != NONE) {
        extern __shared__ float sdata[];
        int tid = threadIdx.x;
        sdata[tid] = thread_sum;
        __syncthreads();

        for (int s = blockDim.x / 2; s &gt; 0; s &gt;&gt;= 1) {
            if (tid &lt; s)
                sdata[tid] += sdata[tid + s];
            __syncthreads();
        }
        if (tid == 0) {
            loss_row[0] = sdata[0];
        }
    }
}

void tv_distance_forward_cpu(const float* p, const float* q,
                             const int* labels, int ignore_index,
                             float* loss, float* grads,
                             int BT, int V,
                             ReductionMode reduction, bool hasLabel)
{
    for (int row = 0; row &lt; BT; ++row) {
        bool ignore = hasLabel &amp;&amp; (labels[row] == ignore_index);
        float row_sum = 0.0f;
        for (int col = 0; col &lt; V; ++col) {
            if (ignore) {
                grads[row * V + col] = 0.0f;
                if (reduction == NONE)
                    loss[row * V + col] = 0.0f;
            } else {
                float p_val = p[row * V + col];
                float q_val = q[row * V + col];
                float tv_loss = 0.5f * fabsf(p_val - q_val);
                float grad = (p_val &gt; q_val) ? 0.5f : -0.5f;
                grads[row * V + col] = grad;
                if (reduction == NONE)
                    loss[row * V + col] = tv_loss;
                else
                    row_sum += tv_loss;
            }
        }
        if (reduction != NONE &amp;&amp; !ignore) {
            loss[row] = row_sum;
        }
    }
}

float post_process_loss(const float* loss, int BT, int V, int n_non_ignore, ReductionMode reduction) {
    float final_loss = 0.0f;
    if (reduction == SUM || reduction == BATCHMEAN || reduction == MEAN) {
        for (int row = 0; row &lt; BT; ++row) {
            final_loss += loss[row];
        }
        if (reduction == BATCHMEAN) {
            final_loss /= n_non_ignore;
        } else if (reduction == MEAN) {
            final_loss /= (n_non_ignore * V);
        }
    }
    return final_loss;
}

void tvd_backward_cpu(const float* grad_output, const float* grads, float* grad_input, int size) {
    float scale = grad_output[0];
    for (int i = 0; i &lt; size; ++i) {
        grad_input[i] = grads[i] * scale;
    }
}

int main() {
    const int BT = 1024;
    const int V = 1024;
    const int ignore_index = -100;
    const ReductionMode reduction = BATCHMEAN;
    const bool hasLabel = true;

    size_t size_mat = BT * V * sizeof(float);
    size_t size_vec = BT * sizeof(float);
    float* h_p = (float*)malloc(size_mat);
    float* h_q = (float*)malloc(size_mat);
    float* h_loss_cpu = (reduction == NONE ? (float*)malloc(size_mat) : (float*)malloc(size_vec));
    float* h_grads_cpu = (float*)malloc(size_mat);
    float* h_grads_back_cpu = (float*)malloc(size_mat);
    int* h_labels = (int*)malloc(BT * sizeof(int));

    for (int i = 0; i &lt; BT * V; ++i) {
        h_p[i] = static_cast&lt;float&gt;(rand()) / RAND_MAX;
        h_q[i] = static_cast&lt;float&gt;(rand()) / RAND_MAX;
    }
    int n_non_ignore = 0;
    for (int i = 0; i &lt; BT; ++i) {
        h_labels[i] = (rand() % 10 == 0) ? ignore_index : i;
        if (h_labels[i] != ignore_index) n_non_ignore++;
    }

    float *d_p, *d_q, *d_loss, *d_grads;
    int* d_labels;
    CUDA_CHECK(cudaMalloc(&amp;d_p, size_mat));
    CUDA_CHECK(cudaMalloc(&amp;d_q, size_mat));
    size_t loss_size = (reduction == NONE ? size_mat : size_vec);
    CUDA_CHECK(cudaMalloc(&amp;d_loss, loss_size));
    CUDA_CHECK(cudaMalloc(&amp;d_grads, size_mat));
    CUDA_CHECK(cudaMalloc(&amp;d_labels, BT * sizeof(int)));

    CUDA_CHECK(cudaMemcpy(d_p, h_p, size_mat, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_q, h_q, size_mat, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_labels, h_labels, BT * sizeof(int), cudaMemcpyHostToDevice));

    int threadsPerBlock = 256;
    int blocks = BT;
    size_t sharedMemSize = threadsPerBlock * sizeof(float);

    cudaEvent_t start, stop;
    CUDA_CHECK(cudaEventCreate(&amp;start));
    CUDA_CHECK(cudaEventCreate(&amp;stop));
    CUDA_CHECK(cudaEventRecord(start));

    tv_distance_kernel&lt;&lt;&lt;blocks, threadsPerBlock, sharedMemSize&gt;&gt;&gt;(
        d_p, V,
        d_q, V,
        d_loss, (reduction == NONE ? V : 1),
        d_grads, V,
        d_labels,
        ignore_index,
        V,
        reduction,
        hasLabel
    );
    CUDA_CHECK(cudaEventRecord(stop));
    CUDA_CHECK(cudaEventSynchronize(stop));
    float ms_gpu = 0;
    CUDA_CHECK(cudaEventElapsedTime(&amp;ms_gpu, start, stop));

    if (reduction == NONE) {
        CUDA_CHECK(cudaMemcpy(h_loss_cpu, d_loss, size_mat, cudaMemcpyDeviceToHost));
    } else {
        CUDA_CHECK(cudaMemcpy(h_loss_cpu, d_loss, size_vec, cudaMemcpyDeviceToHost));
    }
    CUDA_CHECK(cudaMemcpy(h_grads_cpu, d_grads, size_mat, cudaMemcpyDeviceToHost));

    auto cpu_start = std::chrono::high_resolution_clock::now();
    tv_distance_forward_cpu(h_p, h_q, h_labels, ignore_index, h_loss_cpu, h_grads_cpu, BT, V, reduction, hasLabel);
    auto cpu_end = std::chrono::high_resolution_clock::now();
    std::chrono::duration&lt;double, std::milli&gt; ms_cpu = cpu_end - cpu_start;

    float grad_output = 1.0f;
    tvd_backward_cpu(&amp;grad_output, h_grads_cpu, h_grads_back_cpu, BT * V);

    float final_loss = 0.0f;
    if (reduction != NONE) {
        final_loss = post_process_loss(h_loss_cpu, BT, V, n_non_ignore, reduction);
    }

    std::cout &lt;&lt; "GPU kernel time: " &lt;&lt; ms_gpu &lt;&lt; " ms\n";
    std::cout &lt;&lt; "CPU implementation time: " &lt;&lt; ms_cpu.count() &lt;&lt; " ms\n";
    if (reduction != NONE) {
        std::cout &lt;&lt; "Final reduced loss: " &lt;&lt; final_loss &lt;&lt; "\n";
    }

    CUDA_CHECK(cudaFree(d_p));
    CUDA_CHECK(cudaFree(d_q));
    CUDA_CHECK(cudaFree(d_loss));
    CUDA_CHECK(cudaFree(d_grads));
    CUDA_CHECK(cudaFree(d_labels));
    free(h_p);
    free(h_q);
    free(h_loss_cpu);
    free(h_grads_cpu);
    free(h_grads_back_cpu);
    free(h_labels);

    return 0;
}
</code></pre>
    </div>

    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-markdown.min.js"></script>
</body>
</html>